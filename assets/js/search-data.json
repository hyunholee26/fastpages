{
  
    
        "post0": {
            "title": "Pattern Recognition and Machine Learning Summary",
            "content": "1.2 Probability Theory . The probability that $X$ will take the value $x_i$ and $Y$ will take the value $y_j$ is written $p(X = x_i, Y = y_j)$ and is called the joint probability of $X = x_i$ and $Y = y_j$. It is given by the number of points falling in the cell $i,j$ as a fraction of the total number of points, and hence . p(X=xi,Y=yj)=nijNp(X = x_i, Y = y_j) = frac{n_{ij}}{N}p(X=xi​,Y=yj​)=Nnij​​ . p(X=xi)=ciNp(X = x_i) = frac{c_i}{N}p(X=xi​)=Nci​​ . sum rule of probability | . p(X=xi)=∑j=1Lp(X=xi,Y=yj)p(X = x_i) = sum_{j=1}^{L} p(X = x_i, Y = y_j)p(X=xi​)=j=1∑L​p(X=xi​,Y=yj​) . Note that $p(X = x_i)$ is called the marginal probability because it is obtained by marginalizing or summing out the other variables (in this case Y) | . If we consider only those instances for which $X = x_i$, then the fraction of such instances for which $Y = y_j$ is written $p(Y = y_j mid X = x_i)$ and is called the conditional probability of $Y = y_j$ given $X = x_i$. It is obtained by finding the fraction of those points in column $i$ that fall in cell $i,j$ and hence is given by . p(Y=yj∣X=xi)=nijcip(Y = y_j mid X = x_i) = frac{n_{ij}}{c_i}p(Y=yj​∣X=xi​)=ci​nij​​ . product rule of probability | . p(X=xi,Y=yj)=nijN=nijci⋅ciN=p(Y=yj∣X=xi)p(X=xi)p(X = x_i, Y = y_j) = frac{n_{ij}}{N} = frac{n_{ij}}{c_i} cdot frac{c_i}{N} = p(Y=y_j mid X=x_i)p(X = x_i)p(X=xi​,Y=yj​)=Nnij​​=ci​nij​​⋅Nci​​=p(Y=yj​∣X=xi​)p(X=xi​) . More compact notation, . sum rule | . p(X)=∑Yp(X,Y)p(X) = sum_Y p(X,Y)p(X)=Y∑​p(X,Y) . product rule | . p(X,Y)=p(Y∣X)p(X)p(X,Y) = p(Y mid X)p(X)p(X,Y)=p(Y∣X)p(X) . $p(X,Y) = p(Y,X)$ so, we can derive Bayes’ theorem | . p(X,Y)=p(Y∣X)p(X)=p(X∣Y)p(Y)p(X,Y) = p(Y mid X)p(X) = p(X mid Y)p(Y)p(X,Y)=p(Y∣X)p(X)=p(X∣Y)p(Y) . ∴p(Y∣X)=p(X∣Y)p(Y)p(X) therefore p(Y mid X) = frac{p(X mid Y)p(Y)}{p(X)}∴p(Y∣X)=p(X)p(X∣Y)p(Y)​ . Using the sum rule, the denominator in Bayes’ theorem can be expressed in terms of the quantities appearing in the numerator | . p(X)=∑Yp(X,Y)=∑Yp(X∣Y)p(Y)p(X) = sum_Y p(X,Y) = sum_Y p(X mid Y)p(Y)p(X)=Y∑​p(X,Y)=Y∑​p(X∣Y)p(Y) . We can view the denominator in Bayes’ theorem as being the normalization constant required to ensure that the sum of the conditional probability on the left-hand side of over all values of $Y$ equals one. | . 1.2.1 Probability densities . If $x$ and $y$ are two real variables, then the sum and product rules take the form . p(x)=∫p(x,y)dyp(x) = int p(x, y) dyp(x)=∫p(x,y)dy . p(x,y)=p(y∣x)p(x)p(x, y) = p(y mid x)p(x)p(x,y)=p(y∣x)p(x) . 1.2.2 Expectations and covariances . The average value of some function $f(x)$ under a probability distribution $p(x)$ is called the expectation of f(x) and will be denoted by . E[f]=∑xp(x)f(x)  or  E[f]=∫p(x)f(x)dxE[f] = sum_x p(x) f(x) space space or space space E[f] = int p(x) f(x) dxE[f]=x∑​p(x)f(x)  or  E[f]=∫p(x)f(x)dx . If we are given a finite number $N$ of points drawn from the probability distribution or probability density, then the expectation can be approximated as a finite sum over these points | . E[f]∼1N∑n=1Nf(xn)E[f] sim frac{1}{N} sum_{n=1}^{N}f(x_n)E[f]∼N1​n=1∑N​f(xn​) . $E_x[f(x, y)]$ will be a function of y, conditional expectation with respect to a conditional distribution, | . Ex[f∣y]=∑xp(x∣y)f(x)E_x[f mid y] = sum_x p(x mid y)f(x)Ex​[f∣y]=x∑​p(x∣y)f(x) . The variance of $f(x)$ is defined by . var[f]=E[(f(x)−E[f(x)])2]=E[f(x)2]−E[f(x)]2var[f] = E[(f(x) - E[f(x)])^2] = E[f(x)^2] - E[f(x)]^2var[f]=E[(f(x)−E[f(x)])2]=E[f(x)2]−E[f(x)]2 . and provides a measure of how much variability there is in $f(x)$ around its mean value $E[f(x)]$. . | In particular, we can consider the variance of the variable x itself, which is given by . | . var[x]=E[x2]−E[x]2var[x] = E[x^2] - E[x]^2var[x]=E[x2]−E[x]2 . For two random variables $x$ and $y$, the covariance is defined by | . cov[x,y]=Ex,y[(x−E[x])(y−E[y])]=Ex,y[xy]−E[x]E[y]cov[x,y] = E_{x,y}[(x-E[x])(y-E[y])] = E_{x,y}[xy] - E[x]E[y]cov[x,y]=Ex,y​[(x−E[x])(y−E[y])]=Ex,y​[xy]−E[x]E[y] . If x and y are independent, then their covariance vanishes(become 0). . | In the case of two vectors of random variables x and y, the covariance is a matrix . | . cov[x,y]=Ex,y[(x−E[x])(yT−E[yT])]=Ex,y[xyT]−E[x]E[yT]cov[ textbf{x}, textbf{y}] = E_{ textbf{x}, textbf{y}} [( textbf{x} - E[ textbf{x}])( textbf{y}^T -E[ textbf{y}^T])] = E_{ textbf{x}, textbf{y}}[ textbf{x} textbf{y}^T] - E[ textbf{x}]E[ textbf{y}^T]cov[x,y]=Ex,y​[(x−E[x])(yT−E[yT])]=Ex,y​[xyT]−E[x]E[yT] . 1.2.3 Bayesian probabilities . Bayes’ theorem, which takes the form . p(w∣D)=p(D∣w)p(w)p(D)p( textbf{w} mid D) = frac{p(D mid textbf{w})p( textbf{w})}{p(D)}p(w∣D)=p(D)p(D∣w)p(w)​ . then allows us to evaluate the uncertainty in w after we have observed D in the form of the posterior probability $p( textbf{w} mid D)$ . | The quantity $p(D mid textbf{w})$ on the right-hand side of Bayes’ theorem is evaluated for the observed data set $D$ and can be viewed as a function of the parameter vector $ textbf{w}$, in which case it is called the likelihood function. It expresses how probable the observed data set is for different settings of the parameter vector $ textbf{w}$. Note that the likelihood is not a probability distribution over w, and its integral with respect to $ textbf{w}$ does not (necessarily) equal one. . | Given this definition of likelihood, we can state Bayes’ theorem in words . | . posterior∝likelihood×pirorposterior propto likelihood times pirorposterior∝likelihood×piror . where all of these quantities are viewed as functions of $ textbf{w}$. The denominator is the normalization constant, which ensures that the posterior distribution on the left-hand side is a valid probability density and integrates to one. | . p(D)=∫p(D∣w)p(w)dwp(D) = int p(D mid textbf{w})p( textbf{w})d textbf{w}p(D)=∫p(D∣w)p(w)dw . In a frequentist setting, $ textbf{w}$ is considered to be a fixed parameter, whose value is determined by some form of ‘estimator’, and error bars on this estimate are obtained by considering the distribution of possible data sets $D$ . | By contrast, from the Bayesian viewpoint there is only a single data set $D$ (namely the one that is actually observed), and the uncertainty in the parameters is expressed through a probability distribution over $ textbf{w}$. . | . 1.2.4 The Gaussian distribution . For the case of a single real-valued variable $x$, the Gaussian distribution is defined by . N(x∣μ,σ2)=12πσexp(−12(x−μσ)2)N(x mid mu, sigma^2) = frac{1}{ sqrt{2 pi} sigma}exp(- frac{1}{2}( frac{x- mu}{ sigma})^2)N(x∣μ,σ2)=2π . ​σ1​exp(−21​(σx−μ​)2) . which is governed by two parameters: $ mu$, called the mean, and $ sigma^2$, called the variance. The square root of the variance, given by $ sigma$, is called the standard deviation, and the reciprocal of the variance, written as $ beta = 1 / sigma^2$, is called the precision. | . E[x]=∫−∞∞N(x∣μ,σ2)xdx=μE[x] = int_{- infty}^{ infty} N(x mid mu, sigma^2)x dx = muE[x]=∫−∞∞​N(x∣μ,σ2)xdx=μ . E[x2]=∫−∞∞N(x∣μ,σ2)x2dx=μ2+σ2E[x^2] = int_{- infty}^{ infty} N(x mid mu, sigma^2)x^2 dx = mu^2 + sigma^2E[x2]=∫−∞∞​N(x∣μ,σ2)x2dx=μ2+σ2 . (proof) we make the change of variables $u = x - mu$, and get | . E[x]=∫−∞∞12πσexp(−12(uσ)2)(μ+u)duE[x] = int_{- infty}^{ infty} frac{1}{ sqrt{2 pi} sigma}exp(- frac{1}{2}( frac{u}{ sigma})^2)( mu + u) duE[x]=∫−∞∞​2π . ​σ1​exp(−21​(σu​)2)(μ+u)du . E[x]=∫−∞∞12πσexp(−12(uσ)2)μdu+∫−∞∞12πσexp(−12(uσ)2)uduE[x] = int_{- infty}^{ infty} frac{1}{ sqrt{2 pi} sigma}exp(- frac{1}{2}( frac{u}{ sigma})^2) mu du + int_{- infty}^{ infty} frac{1}{ sqrt{2 pi} sigma}exp(- frac{1}{2}( frac{u}{ sigma})^2)u duE[x]=∫−∞∞​2π . ​σ1​exp(−21​(σu​)2)μdu+∫−∞∞​2π . ​σ1​exp(−21​(σu​)2)udu . We see that the first term of the right hand side of the equation is equal to $ mu$ because, it is the normal distribution’s normalization condition multiplied with a constant $ mu$. And we can see by inspection that the second term vanishes because the integrand is an odd function evaluated in $(- infty, infty)$, which is the multiplication of an even function ( $ exp({ frac{-u^2}{2 sigma^2}})$ ) and an odd function ( $u$ ). | . E[x]=u+0E[x] = u + 0E[x]=u+0 . Also, $E[x^2] = sigma^2 + mu^2$ can be proved using $var[x] = E[x^2] - E[x]^2$ | . Gaussian distribution defined over a D-dimensional vector x of continuous variables, which is given by . N(x∣μ,Σ)=1(2π)D∣Σ∣exp(−12(x−μ)TΣ−1(x−μ))N( mathbf{x} mid mathbf{ mu}, mathbf{ Sigma}) = frac{1}{ sqrt{(2 pi)^D lvert mathbf{ Sigma} rvert}} exp (- frac{1}{2}( mathbf{x} - mathbf{ mu})^T mathbf{ Sigma}^{-1} ( mathbf{x} - mathbf{ mu}))N(x∣μ,Σ)=(2π)D∣Σ∣ . ​1​exp(−21​(x−μ)TΣ−1(x−μ)) . where the D-dimensional vector $ mathbf{ mu}$ is called the mean, the $D times D$ matrix $ mathbf{ Sigma}$ is called the covariance, and $ lvert mathbf{ Sigma} rvert$ denotes the determinant of $ Sigma$ . | Data points that are drawn independently from the same distribution are said to be independent and identically distributed, which is often abbreviated to i.i.d. We have seen that the joint probability of two independent events is given by the product of the marginal probabilities for each event separately. Because our data set $ mathbf{x}$ is i.i.d., we can therefore write the probability of the data set, given $ mu$ and $ sigma^2$, in the form . | . p(x∣μ,σ2)=∏n=1NN(xn∣μ,σ2)p( mathbf{x} mid mu, sigma^2) = prod_{n=1}^N N(x_n mid mu, sigma^2)p(x∣μ,σ2)=n=1∏N​N(xn​∣μ,σ2) . When viewed as a function of $ mu$ and $ sigma^2$, this is the likelihood function for the Gaussian . | Taking the log not only simplifies the subsequent mathematical analysis, but it also helps numerically because the product of a large number of small probabilities can easily underflow the numerical precision of the computer, and this is resolved by computing instead the sum of the log probabilities. . | . ln p(x∣μ,σ2)=−12σ2∑n=1N(xn−μ)2−N2lnσ2−N2ln(2π)ln space p( mathbf{x} mid mu, sigma^2) = - frac{1}{2 sigma^2} sum_{n=1}^N (x_n - mu)^2 - frac{N}{2}ln sigma^2 - frac{N}{2}ln(2 pi)ln p(x∣μ,σ2)=−2σ21​n=1∑N​(xn​−μ)2−2N​lnσ2−2N​ln(2π) . Maximizing with respect to $ mu$, we obtain the maximum likelihood solution given by | . μML=1N∑n=1Nxn mu_{ML} = frac{1}{N} sum_{n=1}^N x_nμML​=N1​n=1∑N​xn​ . which is the sample mean, i.e., the mean of the observed values $x_n$. Similarly, maximizing with respect to $ sigma^2$, we obtain the maximum likelihood solution for the variance in the form | . σML2=1N∑n=1N(xn−μML)2 sigma_{ML}^2 = frac{1}{N} sum_{n=1}^N(x_n - mu_{ML})^2σML2​=N1​n=1∑N​(xn​−μML​)2 . which is the sample variance measured with respect to the sample mean $ mu_{ML}$. . | Consider the expectations of these quantities with respect to the data set values, which themselves come from a Gaussian distribution with parameters $ mu$ and $ sigma^2$. It is straightforward to show that . | . E[μML]=μE[ mu_{ML}] = muE[μML​]=μ . E[σML2]=(N−1N)σ2E[ sigma_{ML}^2] = ( frac{N-1}{N}) sigma^2E[σML2​]=(NN−1​)σ2 . -so that on average the maximum likelihood estimate will obtain the correct mean but will underestimate the true variance by a factor $(N − 1)/N$, following estimate for the variance parameter is unbiased . σ~2=NN−1σML2=1N−1∑n=1N(xn−μML)2 tilde{ sigma}^2 = frac{N}{N-1} sigma_{ML}^2 = frac{1}{N-1} sum_{n=1}^N(x_n - mu_{ML})^2σ~2=N−1N​σML2​=N−11​n=1∑N​(xn​−μML​)2 .",
            "url": "https://hyunholee26.github.io/fastpages/machine%20learning/2022/08/30/PRML-Summary.html",
            "relUrl": "/machine%20learning/2022/08/30/PRML-Summary.html",
            "date": " • Aug 30, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Useful expressions in English",
            "content": "2022-03-23 . inform : 목적어로 정보를 전달하고자 하는 사람이 와야한다. inform 사람 of 정보 inform 사람 that My daughter informed me that she was pregnant Please inform me of the result of the game . notify : inform과 형태는 같으나 공식적으로 정보를 전달하는 경우 사용된다 notify 사람 of 정보 notify 사람 that The committee’s decision will be notified to all employees Competition winners will be notified by post . announce : 목적어로 전달하고자 하는 정보가 와야한다. He has announced his intention to retire Please announce to me the result of the game . be familiar with : ~ 를 잘 알다 -&gt; have some knowledge, knowing someone = be acquainted with I am familiar with the reporter. . be used to : ~ 에 익숙하다 -&gt; have some experience = be accustomed to I am not used to eating alone. . 2022-03-12 . How is it going(=coming along)? | I am afraid that it is diffcult to finish it as scheduled(=as we planned). | This is a top priority. Mr.Kyle wants the document this(every, last) week. | After this meeting, I will have Jane finish my other work. | If you have to, do it. | Do not hesitate to contact me if you have any question about(=regarding) this work. | I would appreciate it if you would let me know before you send it to him. tell me : 샅샅히 알려주다 | let me know : 변경사항만 알려주다 | . | Help yourself(많이 퍼가세요, 상황에 맞게 사용해야함), Enjoy your meal 이 오히려 적합할 수 있음 | What are you thinking about? | I was thiking of calling Paul(=give Paul a call, make a phone call). about : 전반적인 사항에 관해 | of : 구체적인 아이디어 | on : 전문적인 사항 | . | I have a presentation on the 29th, and I have to prepare with Paul. prepare : 프로젝트 준비 | get ready : 동작을 할 준비, get ready for dinner, get ready for bed | . | You are lucky. | Paul is good at computers. | Good for you : 잘됐다 | That is good for you : 몸에 좋아 | I am relieved, That is a relief, I am glad : 다행이다 | I was worried about computer work. (a work : 예술작품) | I need to get to know Paul better. get close : get close to each other (물리적 거리) | be close : I am close to Selley. | get to know (someone) | . | That is a good idea but I do not think (that) I can go. | I an going skiing tomorrow, so I have to do a lot of work will : 보통 말하는 순간에 결정할때 사용 | be going to : 예정된 일에 사용 | go (Verb)ing : 활동을 하러가다, go walking, go swimming, go skiing | I did a lot of reading, I did little reading. | . | Do you know any good bars near our office? | I will text(email, katok) you the address later. | I made a reservation for(목적지를 바라보는 전치사) seven o’clock tomorrow at a famous Korean restaurant. make a reservation : 자리를 미리 맡아놓다 | reserve + something : I would like to reserve three tables. | make an appointment : 서비스를 받을 예약을 하다 | . | Have you ever eaten Korean food? | I have never eaten(=had, tried) Korean food | How does it taste? (먹어본 경우) | What is it like(안먹어본경우, 어때요?) | Korean food is usually salty and a little spicy. | Can you eat spicy food? well : 잘 하는것 | . | Yes(Sure, Of course), I can eat everything. =&gt; 남기지 않고 싹 다 먹는다. | Yes, I can eat almost anything. I am not a picky either. : 아무거나 다 먹는다 | I am picky about food. : 입맛이 까다로워요 | That is great, Terrific, Perfect | If you want, I will order for you. instead of : 하나를 포기하는 개념이 포함됨 | on behalf of : ~ 를 대표하여 | . | Tonight, I am going with Selley to the restaurant you recommended last time. will : 말하는 순간에 결정 | be going to : 예정된 미래의 일 | . | The restaurant is very famous for its traditional Korean food. be famous for (소유격) : She is famous for her beautiful voice. | . | I know, I am looking forward toit. | I cannot wait to see you, I am really looking forward to seeing(meeting) you. | Have you ever used chopsticks before? | I used them a few times when I was in Japan. | Are they different from Japanese chopsticks? | What is the difference between Korean chopsticks and Japanese chopsticks? | Korean chopstick are thinner and made of steel. | I am worried that (~할까봐 걱정하다) | I am worried because I haven’t been feeling well these days. | Health is the most important things | What’s the matter? | What’s the problem? | My health is not as good as before | My health is not as good as it used to be | I am not as healthy as I used to be / before | I get tired easily | Do you exercise? work out : 근육운동 | . | Since I started exercising, I have been feeling much(= a lot) better. | I have been exercising these days, and I love it so much. | I quit exercising because of the air is dirty. to 부정사 : ~ 하기 위해 멈추다(미래의미) | -ing : ~ 해 오던 것을 멈추다(과거~미래) | . | I just exercise at home in the house : 물리적인 집 | at home : 나의 생활 공간으로써의 집 | . | Thirty minutes a day is enough. 시간이나 돈의 양은 단수 취급 | . | You’re right, the air pollution is just an excuse. | I should try to lose weight. | I will get started today(on Tuesday) | How have you been, How are you doing? | I heard that you are moving to the US. (제 3자로부터 들었을때) | It all happened so fast. | Things happened so suddenly | Everything happened all of sudden. | I still can’t believe it | I am still trying to wrap my head around it.(아직도 믿기지 않아) | Do you need a help?/ Can I help you? (상대방이 어떤 도움이 필요한지 알고 있을때, 보일때 사용) | Is there anything I can do for you? | Is there anything you need? | I am okay, I am fine : 호의에 대한 거절 | That sounds good. : 좋아! | Thanks for saying that. | Thanks for asking (그렇게 말해줘서 고마워) | Let me know if there’s anything I can do for you (you need) | Don’t hesitate to ask if you need any help. | I hope everty thing goes well. | I wish you the best. | How do you like living in America? | I like the fact that the air is clean. | I like the clean air. | I miss my family and friend | I you have time, When you have time, If you can, When you can | come visit me | I will go see you | I wish I could (go visit you) | I am busy, I have a lot of work. | Have you heard of Squid game? | I have heard a lot about you | It is a lot of fun. (어떤 행동 동작이 재미있는) | It’s really good, It’s really interesting(영화등이 재미있을때) | I am binge-watching it | I am hooked after one episode. | I haven’t seen it yet : 아직 안봤다 see : 어떤 존재를 인지(aware)하다 | watch : 정지상태에서 무엇가를 지켜보다 | . | It seems to be very popular among America. | The show is a big hit in American. | I can’t watch it because I am too busy with(because of를 쓰지 않는다) work and kids. | I am so busy that I can’t watch it | I am too busy to watch it | Why don’t you watch it after putting your kids to bed. make : 억지로/강제로 하다 | . | I am so tired at night that I go to bed at the same time as my kids | That’s too bad. You would like it. : 안타깝다 니가 좋아할텐데 | I got my hair cut. | I got my car washed. | I got my coat cleaned | I got everything done. | I got my suit made. | I have to get my passport picture taken | I will get your watch fixed | Would you mind | . 쓰는영어 .",
            "url": "https://hyunholee26.github.io/fastpages/english/2022/03/12/useful-expressions-in-english.html",
            "relUrl": "/english/2022/03/12/useful-expressions-in-english.html",
            "date": " • Mar 12, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Research Interests . Main research interests are below: . Deep learning for geospatial data such as remote sensed images and DEM (digital elevation model) | Spatiotemporal data mining and applications for water resources management | Spatial data science and geographic information science | . My research focuses on incorporating domain knowledge within data-driven models or learning process as an inductive bias, thereby achieving efficient learning from few samples or sparse observations and making the model easier to understand for scientists and non-machine-learning experts. Also, I would like to research the applications of these techniques in water resources management such as the prediction of water level and dam inflow. For more information about my research experience, please refer to my Curriculum Vitae (PDF). . Education . Arizona State University, Tempe, United State Ph.D in Geographic Information Science | Aug. 2022 - Present | . | KAIST (Korea Advanced Institute of Science and Technology), Daejeon, Republic of Korea M.S in Computer Science | Mar. 2008 – Feb. 2010 | . | Ajou University, Suwon, Republic of Korea B.E in Information and Computer Engineering (intensive major course) | Mar. 2001 – Aug. 2007 | . | . Publication . J Park, H Lee (2020) “Prediction of high turbidity in rivers using LSTM algorithm”. Journal of Korean Society of Water and Wastewater 34 (1), 35-43, https://doi.org/10.11001/jksww.2020.34.1.035 . | J Kim, M Park, Y Yoon, H Lee (2020) “Application of recurrent neural network for inflow prediction into multi-purpose dam basin”. Advances in Hydroinformatics, 397-408, https://doi.org/10.1007/978-981-15-5436-0_31 . | J Park, H Lee, CY Park, S Hasan, TY Heo, WH Lee (2019) “Algal morphological identification in watersheds for drinking water supply using neural architecture search for convolutional neural network”. Water 11 (7), 1338, https://doi.org/10.3390/w11071338 . | H Lee, K Wohn (2010) “The layer-based vector texture for 3D rendering”. Proceeding of 2010 Conference on the HCI Society of Korea, 40-43 . | . Work Experience . Korea Water Resources Corporation (K-water), Daejeon, Korea, Jul. 2010 – Present . Senior Manager, Digital Water Platform Dept., Water Platform Development Team, Jan. 2021 – Jun.2022 | Manager, Digital Innovation Dept., Big Data Business Team, Jan. 2020 – Dec. 2020 | Manager, Data Center Dept., Big Data Business Team, Jan. 2019 – Dec. 2019 | Manager, Water Data Collection and Analysis Dept., Water Data Integration Team, Jan. 2018 – Dec. 2018 | Manager, Human Resources Management Dept., HR Management Team, Jan. 2013 – Dec. 2017 | Staff, Information System Management Dept., Information Planning Team, Jul. 2010 – Dec. 2012 | . | . Honors and Awards . 1st Place Prize, 5th Bigdata analysis competition in K-water, Oct. 2021 . | Academic Conference Paper Award, Korean Society of Environmental Engineering Annual Conference, Nov. 2020 . | Bronze Award, ACM-ICPC (International Collegiate Programming Contest) Asia-Seoul Regional, Nov. 2003 . | . Certification . Advanced Data Analytics Professional, certificated by K-Data, Korea, Apr. 2019 (pass rate: 2.76%) | . Professional Skills . Programming Languages : Python, R, C/C++, JAVA, ABAP (SAP) . | Data Science and Machine Learning : Keras, Tensorflow, Sci-kit Learn . | Visualization : Matplotlib, Plotly, Leaflet, QGIS, OpenGL . | .",
          "url": "https://hyunholee26.github.io/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hyunholee26.github.io/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}