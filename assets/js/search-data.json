{
  
    
        "post0": {
            "title": "Pattern Recognition and Machine Learning Summary",
            "content": "1.2 Probability Theory . The probability that $X$ will take the value $x_i$ and $Y$ will take the value $y_j$ is written $p(X = x_i, Y = y_j)$ and is called the joint probability of $X = x_i$ and $Y = y_j$. It is given by the number of points falling in the cell $i,j$ as a fraction of the total number of points, and hence . p(X=xi,Y=yj)=nijNp(X = x_i, Y = y_j) = frac{n_{ij}}{N}p(X=xi​,Y=yj​)=Nnij​​ . p(X=xi)=ciNp(X = x_i) = frac{c_i}{N}p(X=xi​)=Nci​​ . sum rule of probability | . p(X=xi)=∑j=1Lp(X=xi,Y=yj)p(X = x_i) = sum_{j=1}^{L} p(X = x_i, Y = y_j)p(X=xi​)=j=1∑L​p(X=xi​,Y=yj​) . Note that $p(X = x_i)$ is called the marginal probability because it is obtained by marginalizing or summing out the other variables (in this case Y) | . If we consider only those instances for which $X = x_i$, then the fraction of such instances for which $Y = y_j$ is written $p(Y = y_j mid X = x_i)$ and is called the conditional probability of $Y = y_j$ given $X = x_i$. It is obtained by finding the fraction of those points in column $i$ that fall in cell $i,j$ and hence is given by . p(Y=yj∣X=xi)=nijcip(Y = y_j mid X = x_i) = frac{n_{ij}}{c_i}p(Y=yj​∣X=xi​)=ci​nij​​ . product rule of probability | . p(X=xi,Y=yj)=nijN=nijci⋅ciN=p(Y=yj∣X=xi)p(X=xi)p(X = x_i, Y = y_j) = frac{n_{ij}}{N} = frac{n_{ij}}{c_i} cdot frac{c_i}{N} = p(Y=y_j mid X=x_i)p(X = x_i)p(X=xi​,Y=yj​)=Nnij​​=ci​nij​​⋅Nci​​=p(Y=yj​∣X=xi​)p(X=xi​) . More compact notation, . sum rule | . p(X)=∑Yp(X,Y)p(X) = sum_Y p(X,Y)p(X)=Y∑​p(X,Y) . product rule | . p(X,Y)=p(Y∣X)p(X)p(X,Y) = p(Y mid X)p(X)p(X,Y)=p(Y∣X)p(X) . $p(X,Y) = p(Y,X)$ so, we can derive Bayes’ theorem | . p(X,Y)=p(Y∣X)p(X)=p(X∣Y)p(Y)p(X,Y) = p(Y mid X)p(X) = p(X mid Y)p(Y)p(X,Y)=p(Y∣X)p(X)=p(X∣Y)p(Y) . ∴p(Y∣X)=p(X∣Y)p(Y)p(X) therefore p(Y mid X) = frac{p(X mid Y)p(Y)}{p(X)}∴p(Y∣X)=p(X)p(X∣Y)p(Y)​ . Using the sum rule, the denominator in Bayes’ theorem can be expressed in terms of the quantities appearing in the numerator | . p(X)=∑Yp(X,Y)=∑Yp(X∣Y)p(Y)p(X) = sum_Y p(X,Y) = sum_Y p(X mid Y)p(Y)p(X)=Y∑​p(X,Y)=Y∑​p(X∣Y)p(Y) . We can view the denominator in Bayes’ theorem as being the normalization constant required to ensure that the sum of the conditional probability on the left-hand side of over all values of $Y$ equals one. | . 1.2.1 Probability densities . If $x$ and $y$ are two real variables, then the sum and product rules take the form . p(x)=∫p(x,y)dyp(x) = int p(x, y) dyp(x)=∫p(x,y)dy . p(x,y)=p(y∣x)p(x)p(x, y) = p(y mid x)p(x)p(x,y)=p(y∣x)p(x) . 1.2.2 Expectations and covariances . The average value of some function $f(x)$ under a probability distribution $p(x)$ is called the expectation of f(x) and will be denoted by . E[f]=∑xp(x)f(x)  or  E[f]=∫p(x)f(x)dxE[f] = sum_x p(x) f(x) space space or space space E[f] = int p(x) f(x) dxE[f]=x∑​p(x)f(x)  or  E[f]=∫p(x)f(x)dx . If we are given a finite number $N$ of points drawn from the probability distribution or probability density, then the expectation can be approximated as a finite sum over these points | . E[f]∼1N∑n=1Nf(xn)E[f] sim frac{1}{N} sum_{n=1}^{N}f(x_n)E[f]∼N1​n=1∑N​f(xn​) . $E_x[f(x, y)]$ will be a function of y, conditional expectation with respect to a conditional distribution, | . Ex[f∣y]=∑xp(x∣y)f(x)E_x[f mid y] = sum_x p(x mid y)f(x)Ex​[f∣y]=x∑​p(x∣y)f(x) . The variance of $f(x)$ is defined by . var[f]=E[(f(x)−E[f(x)])2]=E[f(x)2]−E[f(x)]2var[f] = E[(f(x) - E[f(x)])^2] = E[f(x)^2] - E[f(x)]^2var[f]=E[(f(x)−E[f(x)])2]=E[f(x)2]−E[f(x)]2 . and provides a measure of how much variability there is in $f(x)$ around its mean value $E[f(x)]$. . | In particular, we can consider the variance of the variable x itself, which is given by . | . var[x]=E[x2]−E[x]2var[x] = E[x^2] - E[x]^2var[x]=E[x2]−E[x]2 . For two random variables $x$ and $y$, the covariance is defined by | . cov[x,y]=Ex,y[(x−E[x])(y−E[y])]=Ex,y[xy]−E[x]E[y]cov[x,y] = E_{x,y}[(x-E[x])(y-E[y])] = E_{x,y}[xy] - E[x]E[y]cov[x,y]=Ex,y​[(x−E[x])(y−E[y])]=Ex,y​[xy]−E[x]E[y] . If x and y are independent, then their covariance vanishes(become 0). . | In the case of two vectors of random variables x and y, the covariance is a matrix . | . cov[x,y]=Ex,y[(x−E[x])(yT−E[yT])]=Ex,y[xyT]−E[x]E[yT]cov[ textbf{x}, textbf{y}] = E_{ textbf{x}, textbf{y}} [( textbf{x} - E[ textbf{x}])( textbf{y}^T -E[ textbf{y}^T])] = E_{ textbf{x}, textbf{y}}[ textbf{x} textbf{y}^T] - E[ textbf{x}]E[ textbf{y}^T]cov[x,y]=Ex,y​[(x−E[x])(yT−E[yT])]=Ex,y​[xyT]−E[x]E[yT] . 1.2.3 Bayesian probabilities . Bayes’ theorem, which takes the form . p(w∣D)=p(D∣w)p(w)p(D)p( textbf{w} mid D) = frac{p(D mid textbf{w})p( textbf{w})}{p(D)}p(w∣D)=p(D)p(D∣w)p(w)​ . then allows us to evaluate the uncertainty in w after we have observed D in the form of the posterior probability $p( textbf{w} mid D)$ . | The quantity $p(D mid textbf{w})$ on the right-hand side of Bayes’ theorem is evaluated for the observed data set $D$ and can be viewed as a function of the parameter vector $ textbf{w}$, in which case it is called the likelihood function. It expresses how probable the observed data set is for different settings of the parameter vector $ textbf{w}$. Note that the likelihood is not a probability distribution over w, and its integral with respect to $ textbf{w}$ does not (necessarily) equal one. . | Given this definition of likelihood, we can state Bayes’ theorem in words . | . posterior∝likelihood×pirorposterior propto likelihood times pirorposterior∝likelihood×piror . where all of these quantities are viewed as functions of $ textbf{w}$. The denominator is the normalization constant, which ensures that the posterior distribution on the left-hand side is a valid probability density and integrates to one. | . p(D)=∫p(D∣w)p(w)dwp(D) = int p(D mid textbf{w})p( textbf{w})d textbf{w}p(D)=∫p(D∣w)p(w)dw . In a frequentist setting, $ textbf{w}$ is considered to be a fixed parameter, whose value is determined by some form of ‘estimator’, and error bars on this estimate are obtained by considering the distribution of possible data sets $D$ . | By contrast, from the Bayesian viewpoint there is only a single data set $D$ (namely the one that is actually observed), and the uncertainty in the parameters is expressed through a probability distribution over $ textbf{w}$. . | . 1.2.4 The Gaussian distribution . For the case of a single real-valued variable $x$, the Gaussian distribution is defined by . N(x∣μ,σ2)=12πσexp(−12(x−μσ)2)N(x mid mu, sigma^2) = frac{1}{ sqrt{2 pi} sigma}exp(- frac{1}{2}( frac{x- mu}{ sigma})^2)N(x∣μ,σ2)=2π . ​σ1​exp(−21​(σx−μ​)2) . which is governed by two parameters: $ mu$, called the mean, and $ sigma^2$, called the variance. The square root of the variance, given by $ sigma$, is called the standard deviation, and the reciprocal of the variance, written as $ beta = 1 / sigma^2$, is called the precision. | . E[x]=∫−∞∞N(x∣μ,σ2)xdx=μE[x] = int_{- infty}^{ infty} N(x mid mu, sigma^2)x dx = muE[x]=∫−∞∞​N(x∣μ,σ2)xdx=μ . E[x2]=∫−∞∞N(x∣μ,σ2)x2dx=μ2+σ2E[x^2] = int_{- infty}^{ infty} N(x mid mu, sigma^2)x^2 dx = mu^2 + sigma^2E[x2]=∫−∞∞​N(x∣μ,σ2)x2dx=μ2+σ2 . (proof) we make the change of variables $u = x - mu$, and get | . E[x]=∫−∞∞12πσexp(−12(uσ)2)(μ+u)duE[x] = int_{- infty}^{ infty} frac{1}{ sqrt{2 pi} sigma}exp(- frac{1}{2}( frac{u}{ sigma})^2)( mu + u) duE[x]=∫−∞∞​2π . ​σ1​exp(−21​(σu​)2)(μ+u)du . E[x]=∫−∞∞12πσexp(−12(uσ)2)μdu+∫−∞∞12πσexp(−12(uσ)2)uduE[x] = int_{- infty}^{ infty} frac{1}{ sqrt{2 pi} sigma}exp(- frac{1}{2}( frac{u}{ sigma})^2) mu du + int_{- infty}^{ infty} frac{1}{ sqrt{2 pi} sigma}exp(- frac{1}{2}( frac{u}{ sigma})^2)u duE[x]=∫−∞∞​2π . ​σ1​exp(−21​(σu​)2)μdu+∫−∞∞​2π . ​σ1​exp(−21​(σu​)2)udu . We see that the first term of the right hand side of the equation is equal to $ mu$ because, it is the normal distribution’s normalization condition multiplied with a constant $ mu$. And we can see by inspection that the second term vanishes because the integrand is an odd function evaluated in $(- infty, infty)$, which is the multiplication of an even function ( $ exp({ frac{-u^2}{2 sigma^2}})$ ) and an odd function ( $u$ ). | . E[x]=u+0E[x] = u + 0E[x]=u+0 . Also, $E[x^2] = sigma^2 + mu^2$ can be proved using $var[x] = E[x^2] - E[x]^2$ | . Gaussian distribution defined over a D-dimensional vector x of continuous variables, which is given by . N(x∣μ,Σ)=1(2π)D∣Σ∣exp(−12(x−μ)TΣ−1(x−μ))N( mathbf{x} mid mathbf{ mu}, mathbf{ Sigma}) = frac{1}{ sqrt{(2 pi)^D lvert mathbf{ Sigma} rvert}} exp (- frac{1}{2}( mathbf{x} - mathbf{ mu})^T mathbf{ Sigma}^{-1} ( mathbf{x} - mathbf{ mu}))N(x∣μ,Σ)=(2π)D∣Σ∣ . ​1​exp(−21​(x−μ)TΣ−1(x−μ)) . where the D-dimensional vector $ mathbf{ mu}$ is called the mean, the $D times D$ matrix $ mathbf{ Sigma}$ is called the covariance, and $ lvert mathbf{ Sigma} rvert$ denotes the determinant of $ Sigma$ . | Data points that are drawn independently from the same distribution are said to be independent and identically distributed, which is often abbreviated to i.i.d. We have seen that the joint probability of two independent events is given by the product of the marginal probabilities for each event separately. Because our data set $ mathbf{x}$ is i.i.d., we can therefore write the probability of the data set, given $ mu$ and $ sigma^2$, in the form . | . p(x∣μ,σ2)=∏n=1NN(xn∣μ,σ2)p( mathbf{x} mid mu, sigma^2) = prod_{n=1}^N N(x_n mid mu, sigma^2)p(x∣μ,σ2)=n=1∏N​N(xn​∣μ,σ2) . When viewed as a function of $ mu$ and $ sigma^2$, this is the likelihood function for the Gaussian . | Taking the log not only simplifies the subsequent mathematical analysis, but it also helps numerically because the product of a large number of small probabilities can easily underflow the numerical precision of the computer, and this is resolved by computing instead the sum of the log probabilities. . | . ln p(x∣μ,σ2)=−12σ2∑n=1N(xn−μ)2−N2lnσ2−N2ln(2π)ln space p( mathbf{x} mid mu, sigma^2) = - frac{1}{2 sigma^2} sum_{n=1}^N (x_n - mu)^2 - frac{N}{2}ln sigma^2 - frac{N}{2}ln(2 pi)ln p(x∣μ,σ2)=−2σ21​n=1∑N​(xn​−μ)2−2N​lnσ2−2N​ln(2π) . Maximizing with respect to $ mu$, we obtain the maximum likelihood solution given by | . μML=1N∑n=1Nxn mu_{ML} = frac{1}{N} sum_{n=1}^N x_nμML​=N1​n=1∑N​xn​ . which is the sample mean, i.e., the mean of the observed values $x_n$. Similarly, maximizing with respect to $ sigma^2$, we obtain the maximum likelihood solution for the variance in the form | . σML2=1N∑n=1N(xn−μML)2 sigma_{ML}^2 = frac{1}{N} sum_{n=1}^N(x_n - mu_{ML})^2σML2​=N1​n=1∑N​(xn​−μML​)2 . which is the sample variance measured with respect to the sample mean $ mu_{ML}$. . | Consider the expectations of these quantities with respect to the data set values, which themselves come from a Gaussian distribution with parameters $ mu$ and $ sigma^2$. It is straightforward to show that . | . E[μML]=μE[ mu_{ML}] = muE[μML​]=μ . E[σML2]=(N−1N)σ2E[ sigma_{ML}^2] = ( frac{N-1}{N}) sigma^2E[σML2​]=(NN−1​)σ2 . -so that on average the maximum likelihood estimate will obtain the correct mean but will underestimate the true variance by a factor $(N − 1)/N$, following estimate for the variance parameter is unbiased . σ~2=NN−1σML2=1N−1∑n=1N(xn−μML)2 tilde{ sigma}^2 = frac{N}{N-1} sigma_{ML}^2 = frac{1}{N-1} sum_{n=1}^N(x_n - mu_{ML})^2σ~2=N−1N​σML2​=N−11​n=1∑N​(xn​−μML​)2 .",
            "url": "https://hyunholee26.github.io/fastpages/machine%20learning/2022/08/30/PRML-Summary.html",
            "relUrl": "/machine%20learning/2022/08/30/PRML-Summary.html",
            "date": " • Aug 30, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Research Interests . Main research interests are below: . Deep learning for geospatial data such as remote sensed images and DEM (digital elevation model) | Spatiotemporal data mining and applications for water resources management | Spatial data science and geographic information science | . My research focuses on incorporating domain knowledge within data-driven models or learning process as an inductive bias, thereby achieving efficient learning from few samples or sparse observations and making the model easier to understand for scientists and non-machine-learning experts. Also, I would like to research the applications of these techniques in water resources management such as the prediction of water level and dam inflow. For more information about my research experience, please refer to my Curriculum Vitae (PDF). . Education . Arizona State University, Tempe, United State Ph.D in Geographic Information Science | Aug. 2022 - Present | . | KAIST (Korea Advanced Institute of Science and Technology), Daejeon, Republic of Korea M.S in Computer Science | Mar. 2008 – Feb. 2010 | . | Ajou University, Suwon, Republic of Korea B.E in Information and Computer Engineering (intensive major course) | Mar. 2001 – Aug. 2007 | . | . Publication . J Park, H Lee (2020) “Prediction of high turbidity in rivers using LSTM algorithm”. Journal of Korean Society of Water and Wastewater 34 (1), 35-43, https://doi.org/10.11001/jksww.2020.34.1.035 . | J Kim, M Park, Y Yoon, H Lee (2020) “Application of recurrent neural network for inflow prediction into multi-purpose dam basin”. Advances in Hydroinformatics, 397-408, https://doi.org/10.1007/978-981-15-5436-0_31 . | J Park, H Lee, CY Park, S Hasan, TY Heo, WH Lee (2019) “Algal morphological identification in watersheds for drinking water supply using neural architecture search for convolutional neural network”. Water 11 (7), 1338, https://doi.org/10.3390/w11071338 . | H Lee, K Wohn (2010) “The layer-based vector texture for 3D rendering”. Proceeding of 2010 Conference on the HCI Society of Korea, 40-43 . | . Work Experience . Korea Water Resources Corporation (K-water), Daejeon, Korea, Jul. 2010 – Present . Senior Manager, Digital Water Platform Dept., Water Platform Development Team, Jan. 2021 – Jun.2022 | Manager, Digital Innovation Dept., Big Data Business Team, Jan. 2020 – Dec. 2020 | Manager, Data Center Dept., Big Data Business Team, Jan. 2019 – Dec. 2019 | Manager, Water Data Collection and Analysis Dept., Water Data Integration Team, Jan. 2018 – Dec. 2018 | Manager, Human Resources Management Dept., HR Management Team, Jan. 2013 – Dec. 2017 | Staff, Information System Management Dept., Information Planning Team, Jul. 2010 – Dec. 2012 | . | . Honors and Awards . 1st Place Prize, 5th Bigdata analysis competition in K-water, Oct. 2021 . | Academic Conference Paper Award, Korean Society of Environmental Engineering Annual Conference, Nov. 2020 . | Bronze Award, ACM-ICPC (International Collegiate Programming Contest) Asia-Seoul Regional, Nov. 2003 . | . Certification . Advanced Data Analytics Professional, certificated by K-Data, Korea, Apr. 2019 (pass rate: 2.76%) | . Professional Skills . Programming Languages : Python, R, C/C++, JAVA, ABAP (SAP) . | Data Science and Machine Learning : Keras, Tensorflow, Sci-kit Learn . | Visualization : Matplotlib, Plotly, Leaflet, QGIS, OpenGL . | .",
          "url": "https://hyunholee26.github.io/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hyunholee26.github.io/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}