{
  
    
        "post0": {
            "title": "Steve Brunton's talks in youtube",
            "content": "1. Sparse Identification of Nonlinear Dynamics (SINDy): Sparse Machine Learning Models 5 Years Later! . I will summarize it later | . 2. Deep Learning to Discover Coordinates for Dynamics: Autoencoders &amp; Physics Informed Machine Learning . I will summarize it later | .",
            "url": "https://hyunholee26.github.io/fastpages/physics%20guided%20deeplearning/2022/02/24/Steve-Brunton-talks-in-youtube.html",
            "relUrl": "/physics%20guided%20deeplearning/2022/02/24/Steve-Brunton-talks-in-youtube.html",
            "date": " • Feb 24, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Rainfall Runoff Modeling using Recurrent Neural Network",
            "content": "1. Rainfall–runoff modelling using Long Short-Term Memory (LSTM) networks . Authors : Frederik Kratzert, Daniel Klotz, Claire Brenner, Karsten Schulz, and Mathew Herrnegger | Abstract : Rainfall–runoff modelling is one of the key challenges in the field of hydrology. Various approaches exist, ranging from physically based over conceptual to fully data-driven models. In this paper, we propose a novel data-driven approach, using the Long Short-Term Memory (LSTM) network, a special type of recurrent neural network. The advantage of the LSTM is its ability to learn long-term dependencies between the provided input and output of the network, which are essential for modelling storage effects in e.g. catchments with snow influence. We use 241 catchments of the freely available CAMELS data set to test our approach and also compare the results to the well-known Sacramento Soil Moisture Accounting Model (SAC-SMA) coupled with the Snow-17 snow routine. We also show the potential of the LSTM as a regional hydrological model in which one model predicts the discharge for a variety of catchments. In our last experiment, we show the possibility to transfer process understanding, learned at regional scale, to individual catchments and thereby increasing model performance when compared to a LSTM trained only on the data of single catchments. Using this approach, we were able to achieve better model performance as the SAC-SMA + Snow-17, which underlines the potential of the LSTM for hydrological modelling applications. | My Notes : Figure 15 shows the evolution of a single LSTM cell (ct; see Sect. 2.1) of a trained LSTM over the period of one input sequence (which equals 365 days in this study) for an arbitrary, snow-influenced catchment. We can see that the cell state matches the dynamics of the temperature curves, as well as our understanding of snow accumulation and snowmelt. As soon as temperatures fall below 0 ∘C the cell state starts to increase (around time step 60) until the minimum temperature increases above the freezing point (around time step 200) and the cell state depletes quickly. Also, the fluctuations between time steps 60 and 120 match the fluctuations visible in the temperature around the freezing point. Thus, albeit the LSTM was only trained to predict runoff from meteorological observations, it has learned to model snow dynamics without any forcing to do so. | . . 2. Sequence-to-Sequence Learning with Deep Neural Networks in Rainfall-Runoff Modeling in Iowa . Authors : Xiang, Z., Demir, I. | Abstract : Sequence-to-sequence(seq2seq) learning with deep neural networks can be used to solve complex time-series problems. This study presents an end-to-end rainfall-runoff model considering the NCEP/CPC 4km precipitation, empirical evapotranspiration, and USGS stream runoff data in watershed scale using seq2seq learning with Gated Recurrent Unit network. For each USGS station-based watershed, after calibration with appropriate input timesteps and batch size determined from the domain, the model can predict runoff for the next 120 hours with observed precipitation and runoff, empirical evapotranspiration, and forecast precipitation. For the downstream stations, we used the upstream forecast results as an additional input, which reduced the errors caused by spatial inequality of precipitation and improved the model accuracy. Final evaluation shows that, for the test water year 2018, on 126 available USGS stations in Iowa, the median of Nash-Sutcliffe model Efficiency (NSE) are 0.84, 0.77, 0.72, 0.69 and 0.67 for the prediction of 24th, 48th, 72nd, 96th and 120th hour, while the stream persistences are 0.74, 0.40, 0.11, -0.02 and -0.15 respectively. 98 out of 126 stations have an acceptable 120th-hour prediction NSE value over 0.50. The results show strong predictive power and could be used to improve forecast accuracy in short-term flood forecast applications. This study also demonstrates a strong potential of applying seq2seq learning on time-series tasks in hydrology and earth science studies. | My Notes : For each USGS station-based watershed, after calibration with appropriate input timesteps and batch size determined from the domain, the model can predict runoff for the next 120 hours with observed precipitation and runoff, empirical evapotranspiration, and forecast precipitation. For the downstream stations, we used the upstream forecast results as an additional input, which reduced the errors caused by spatial inequality of precipitation and improved the model accuracy. | . 3. A Rainfall-Runoff Model With LSTM-Based Sequence-to-Sequence Learning . Authors : Zhongrun Xiang, Jun Yan, Ibrahim Demir | Abstract : Rainfall-runoff modeling is a complex nonlinear time series problem. While there is still room for improvement, researchers have been developing physical and machine learning models for decades to predict runoff using rainfall data sets. With the advancement of computational hardware resources and algorithms, deep learning methods such as the long short-term memory (LSTM) model and sequence-to-sequence (seq2seq) modeling have shown a good deal of promise in dealing with time series problems by considering long-term dependencies and multiple outputs. This study presents an application of a prediction model based on LSTM and the seq2seq structure to estimate hourly rainfall-runoff. Focusing on two Midwestern watersheds, namely, Clear Creek and Upper Wapsipinicon River in Iowa, these models were used to predict hourly runoff for a 24-hr period using rainfall observation, rainfall forecast, runoff observation, and empirical monthly evapotranspiration data from all stations in these two watersheds. The models were evaluated using the Nash-Sutcliffe efficiency coefficient, the correlation coefficient, statistical bias, and the normalized root-mean-square error. The results show that the LSTM-seq2seq model outperforms linear regression, Lasso regression, Ridge regression, support vector regression, Gaussian processes regression, and LSTM in all stations from these two watersheds. The LSTM-seq2seq model shows sufficient predictive power and could be used to improve forecast accuracy in short-term flood forecast applications. In addition, the seq2seq method was demonstrated to be an effective method for time series predictions in hydrology. | .",
            "url": "https://hyunholee26.github.io/fastpages/rainfall-runoff%20modeling/recurrent%20neural%20network/2022/02/20/Rainfall-Runoff-Modeling-using-Recurrent-Neural-Network.html",
            "relUrl": "/rainfall-runoff%20modeling/recurrent%20neural%20network/2022/02/20/Rainfall-Runoff-Modeling-using-Recurrent-Neural-Network.html",
            "date": " • Feb 20, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Rainfall Runoff Modeling using Graph Neural Network",
            "content": "1. High-resolution rainfall-runoff modeling using graph neural network . Authors : Zhongrun Xiang, Ibrahim Demir | Abstract : Time-series modeling has shown great promise in recent studies using the latest deep learning algorithms such as LSTM (Long Short-Term Memory). These studies primarily focused on watershed-scale rainfall-runoff modeling or streamflow forecasting, but the majority of them only considered a single watershed as a unit. Although this simplification is very effective, it does not take into account spatial information, which could result in significant errors in large watersheds. Several studies investigated the use of GNN (Graph Neural Networks) for data integration by decomposing a large watershed into multiple sub-watersheds, but each sub-watershed is still treated as a whole, and the geoinformation contained within the watershed is not fully utilized. In this paper, we propose the GNRRM (Graph Neural Rainfall-Runoff Model), a novel deep learning model that makes full use of spatial information from high-resolution precipitation data, including flow direction and geographic information. When compared to baseline models, GNRRM has less over-fitting and significantly improves model performance. Our findings support the importance of hydrological data in deep learning-based rainfall-runoff modeling, and we encourage researchers to include more domain knowledge in their models. | . 2. Fully distributedrainfall-runoff modeling using spatial-temporal graph neural network . Authors : Zhongrun Xiang, Ibrahim Demir | Abstract : Recent studies using latest deep learning algorithms such as LSTM (Long Short-Term Memory) have shown great promise in time-series modeling. There are many studies focusing on the watershed-scale rainfall-runoff modeling or streamflow forecasting, often considering a single watershed with limited generalization capabilities. To improve the model performance, several studies explored an integrated approach by decomposing a large watershed into multiple sub-watersheds with semi-distributed structure. In this study, we propose an innovative physics-informed fully-distributed rainfall-runoff model, NRM-Graph (Neural Runoff Model-Graph), using Graph Neural Networks (GNN) to make full use of spatial information including the flow direction and geographic data. Specifically, we applied a time-series model on each grid cell for its runoff production. The output of each grid cell is then aggregated by a GNN as the final runoff at the watershed outlet. The case study shows that our GNN based model successfully represents the spatial information in predictions. NRM-Graph network has shown less over-fitting and a significant improvement on the model performance compared to the baselines with spatial information. Our research further confirms the importance of spatially distributed hydrological information in rainfall-runoff modeling using deep learning, and we encourage researchers to incorporate more domain knowledge in modeling. | . 3. Large-scale river network modeling using Graph Neural Networks . Authors : Frederik Kratzert, Daniel Klotz, Martin Gauch, Christoph Klingler, Grey Nearing, and Sepp Hochreiter | Abstract : In the recent past, several studies have demonstrated the ability of deep learning (DL) models, especially based on Long Short-Term Memory (LSTM) networks, for rainfall-runoff modeling. However, almost all of these studies were limited to (multiple) individual catchments or small river networks, consisting of only a few connected catchments. In this study, we investigate large-scale, spatially distributed rainfall-runoff modeling using DL models. Our setup consists of two independent model components: One model for the runoff-generation process and one for the routing. The former is an LSTM-based model that predicts the discharge contribution of each sub-catchment in a river network. The latter is a Graph Neural Network (GNN) that routes the water along the river network network in hierarchical order. The first part is set up to simulate unimpaired runoff for every sub-catchment. Then, the GNN routes the water through the river network, incorporating human influences such as river regulations through hydropower plants. The main focus is to investigate different model architectures for the GNN that are able to learn the routing task, as well as potentially accounting for human influence. We consider models based on 1D-convolution, attention modules, as well as state-aware time series models.The decoupled approach with individual models for sub-catchment discharge prediction and routing has several benefits: a) We have an intermediate output of per-basin discharge contributions that we can inspect. b) We can leverage observed streamflow when available. That is, we can optionally substitute the discharge simulations of the first model with observed discharge, to make use of as much observed information as possible. c) We can train the model very efficiently. d) We can simulate any intermediate node in the river network, without requiring discharge observations.For the experiments, we use a new large-sample dataset called LamaH (Large-sample Data for Hydrology in Central Europe) that covers all of Austria and the foreign upstream areas of the Danube. We consider the entire Danube catchment upstream of Bratislava, a highly diverse region, including large parts of the Alps, that covers a total area of more than 130000km2. Within that area, LamaH contains hourly and daily discharge observations for more than 600 gauge stations. Thus, we investigate DL-based routing models not only for daily discharge, but also for hourly discharge.Our first results are promising, both daily and hourly discharge simulation. For example, the fully DL-based distributed models capture the dynamics as well as the timing of the devastating 2002 Danube flood. Building upon our work on learning universal, regional, and local hydrological behaviors with machine learning, we try to make the GNN-based routing as universal as possible, striving towards a globally applicable, spatially distributed, fully learned hydrological model. | ppt slide | . 4. Short-term Hourly Streamflow Prediction with Graph Convolutional GRU Networks . Authors : Muhammed Sit, Bekir Demiray, Ibrahim Demir | Abstract : The frequency and impact of floods are expected to increase due to climate change. It is crucial to predict streamflow, consequently flooding, in order to prepare and mitigate its consequences in terms of property damage and fatalities. This paper presents a Graph Convolutional GRUs based model to predict the next 36 hours of streamflow for a sensor location using the upstream river network. As shown in experiment results, the model presented in this study provides better performance than the persistence baseline and a Convolutional Bidirectional GRU network for the selected study area in short-term streamflow prediction. | .",
            "url": "https://hyunholee26.github.io/fastpages/graph%20neural%20network/rainfall-runoff%20modeling/2022/02/20/Rainfall-Runoff-Modeling-using-Graph-Neural-Network.html",
            "relUrl": "/graph%20neural%20network/rainfall-runoff%20modeling/2022/02/20/Rainfall-Runoff-Modeling-using-Graph-Neural-Network.html",
            "date": " • Feb 20, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "A Comprehensive Survey on Graph Neural Networks",
            "content": "0. Paper Link . A Comprehensive Survey on Graph Neural Networks . 1. Background . This paper will be summarized after reviewing… .",
            "url": "https://hyunholee26.github.io/fastpages/graph%20neural%20network/2022/02/20/A-Comprehensive-Survey-on-Graph-Neural-Networks.html",
            "relUrl": "/graph%20neural%20network/2022/02/20/A-Comprehensive-Survey-on-Graph-Neural-Networks.html",
            "date": " • Feb 20, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Regarding Deep learning with Uncertainty Estimation",
            "content": "I will summarize serveral articles regarding deep learning with uncertainty estimation. Providing confidence intervals in regression problems is related to uncertainty estimation. A phrase that can express this well is quoted as follows. . We do a lot of regression, with everything from random forests to recurrent neural networks. And as good as our models are, we know they can never be perfect. Therefore, whenever we provide our customers with predictions, we also like to include a set of confidence intervals: what range around the prediction will the actual value fall within, with (e.g.) 80% confidence? . 1. A guide to generating probability distributions with neural networks . This article give the example code related to generate distribution of predicted value with cumstomed layer and loss function. | . 2. How to generate neural network confidence intervals with Keras . This article suggests same notion and implementation with Monte Carlo Dropout. Also, it gives the way to calculate optimal dropout rate. | .",
            "url": "https://hyunholee26.github.io/fastpages/uncertainty%20estimation/2022/02/13/Regarding-Deep-learning-with-Uncertainty-Estimation.html",
            "relUrl": "/uncertainty%20estimation/2022/02/13/Regarding-Deep-learning-with-Uncertainty-Estimation.html",
            "date": " • Feb 13, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Simclr",
            "content": "A Simple Framework for Contrastive Learning of Visual Representations .",
            "url": "https://hyunholee26.github.io/fastpages/2022/02/06/simclr.html",
            "relUrl": "/2022/02/06/simclr.html",
            "date": " • Feb 6, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Building own blog with Jekyll and Github in 20 minutes",
            "content": "Step 0. Pre-requisites . Join Github | Select and download Jekyll theme in http://jekyllthemes.org/ | Create new repository and set it for homepage, and then upload the files of Jekyll theme into the new repository | . Step 1. Edit /_config.yml . This file is relevant to theme, Site settings(title, logo, etc), Site Author(name, bio, social links) | . Step 2. Edit ‘/_data/navigation.yml’ . This file is relevant to menus and its link | Each menus consists of title and url. title is the name of menu and url means a file located in _pages folder. You can set the file with checking the example at /test/_data/navigation.yml | . Step 3. Create ‘/_page’ folder and menu files . This is relevant to creating menu files. This step is connected with previous step. | Menu file could be .md or .html and you can make the files with refering to the example at /test/_pages | . Step 4. Create ‘/_post’ folder and .md files . This is relevant to posting an article in your blog | The name of .md file format must be YYYY-MM-DD-POST-TITLE.md and you can compose the article with markdown. | . Step 5. That’s it . You can check your blog and posts that you wrote. | .",
            "url": "https://hyunholee26.github.io/fastpages/github/jekyll/2022/02/06/Building-own-blog-with-jekyll-and-github-in-20-minutes.html",
            "relUrl": "/github/jekyll/2022/02/06/Building-own-blog-with-jekyll-and-github-in-20-minutes.html",
            "date": " • Feb 6, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://hyunholee26.github.io/fastpages/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Research Interests . Main research interests are below: . Physics-guided machine learning | Spatiotemporal data mining (especially forecasting) | Uncertainty Estimation | . My research focuses on incorporating prior knowledge (especially physics-based knowledge) within data-driven models or learning process as an inductive bias, thereby achieving efficient learning from few samples or sparse observations and making the model easier to understand for scientists and non-machine-learning experts. Also, I would like to research the applications of these techniques in water resources management such as the prediction of water level and dam inflow. For more information about my research experience, please refer to my Curriculum Vitae (PDF). . Education . KAIST (Korea Advanced Institute of Science and Technology), Daejeon, Republic of Korea M.S in Computer Science | Mar. 2008 – Feb. 2010 | . | Ajou University, Suwon, Republic of Korea B.E in Information and Computer Engineering (intensive major course) | Mar. 2001 – Aug. 2007 | . | . Publication . J Park, H Lee (2020) “Prediction of high turbidity in rivers using LSTM algorithm”. Journal of Korean Society of Water and Wastewater 34 (1), 35-43, https://doi.org/10.11001/jksww.2020.34.1.035 . | J Kim, M Park, Y Yoon, H Lee (2020) “Application of recurrent neural network for inflow prediction into multi-purpose dam basin”. Advances in Hydroinformatics, 397-408, https://doi.org/10.1007/978-981-15-5436-0_31 . | J Park, H Lee, CY Park, S Hasan, TY Heo, WH Lee (2019) “Algal morphological identification in watersheds for drinking water supply using neural architecture search for convolutional neural network”. Water 11 (7), 1338, https://doi.org/10.3390/w11071338 . | H Lee, K Wohn (2010) “The layer-based vector texture for 3D rendering”. Proceeding of 2010 Conference on the HCI Society of Korea, 40-43 . | . Work Experience . Korea Water Resources Corporation (K-water), Daejeon, Korea, Jul. 2010 – Present . Senior Manager, Digital Water Platform Dept., Water Platform Development Team, Jan. 2021 – Jun.2022 | Manager, Digital Innovation Dept., Big Data Business Team, Jan. 2020 – Dec. 2020 | Manager, Data Center Dept., Big Data Business Team, Jan. 2019 – Dec. 2019 | Manager, Water Data Collection and Analysis Dept., Water Data Integration Team, Jan. 2018 – Dec. 2018 | Manager, Human Resources Management Dept., HR Management Team, Jan. 2013 – Dec. 2017 | Staff, Information System Management Dept., Information Planning Team, Jul. 2010 – Dec. 2012 | . | . Honors and Awards . 1st Place Prize, 5th Bigdata analysis competition in K-water, Oct. 2021 . | Academic Conference Paper Award, Korean Society of Environmental Engineering Annual Conference, Nov. 2020 . | Bronze Award, ACM-ICPC (International Collegiate Programming Contest) Asia-Seoul Regional, Nov. 2003 . | . Certification . Advanced Data Analytics Professional, certificated by K-Data, Korea, Apr. 2019 (pass rate: 2.76%) | . Professional Skills . Programming Languages : Python, R, C/C++, JAVA, ABAP (SAP) . | Data Science and Machine Learning : Keras, Tensorflow, Sci-kit Learn . | Visualization : Matplotlib, Plotly, Leaflet, QGIS, OpenGL . | .",
          "url": "https://hyunholee26.github.io/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hyunholee26.github.io/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}