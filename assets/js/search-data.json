{
  
    
        "post0": {
            "title": "Understanding Gaussian Process without any knowledge (아무 지식이 없는 상태에서 Gaussian Process 까지 이해해보기)",
            "content": "1. Conditional probability . In probability theory, conditional probability is a measure of the probability of an event occurring, given that another event (by assumption, presumption, assertion or evidence) has already occurred. . P(A∣B)=P(A∩B)P(B)P(A | B) = frac{P( A cap B)}{P(B)}P(A∣B)=P(B)P(A∩B)​ . Independence(독립) : Two events A and B are independent if and only if their joint probability equals the product of their probabilities | . P(A∩B)=P(A)⋅P(B)P(A cap B) = P(A) cdot P(B)P(A∩B)=P(A)⋅P(B) . 조건부 확률에서 P(A)와 P(B)가 독립인 경우, $P(A | B) = frac{P( A cap B)}{P(B)} = frac{P(A) cdot P(B)}{P(B)} = P(A) $ 가 성립하며, 조건부 확률로 사건A에 대해 사건B가 주어지는 경우와 주어지지 않는 경우의 확률이 같은 경우를 의미하는 것으로 이해할 수 있다. 또한 다른말로 표현해보면, 전체에서 A가 발생할 확률과 사건B가 발생했을 때 사건A가 발생할 확률이 같은 경우를 의미하는 것으로도 이해할 수 있다. | . 2. Bayes’s Theorem . Bayes’ theorem is stated mathematically as the following equation: | . P(A∣B)=P(B∣A)⋅P(A)P(B),where P(B)≠0.P(A|B) = frac {P(B|A) cdot P(A)} {P(B)}, where space P(B) neq 0.P(A∣B)=P(B)P(B∣A)⋅P(A)​,where P(B)=0. . Bayes’s Theorem은 conditonal probability로 부터 유도됩니다. | . P(A∣B)=P(A∩B)P(B), then P(A∩B)=P(A∣B)⋅P(B)P(A | B) = frac{P( A cap B)}{P(B)}, space then space P(A cap B) = P(A | B) cdot P(B)P(A∣B)=P(B)P(A∩B)​, then P(A∩B)=P(A∣B)⋅P(B) . P(B∣A)=P(A∩B)P(A), then P(A∩B)=P(B∣A)⋅P(A)P(B | A) = frac{P( A cap B)}{P(A)}, space then space P(A cap B) = P(B | A) cdot P(A)P(B∣A)=P(A)P(A∩B)​, then P(A∩B)=P(B∣A)⋅P(A) . so,P(A∣B)⋅P(B)=P(B∣A)⋅P(A)so, P(A | B) cdot P(B) = P(B | A) cdot P(A)so,P(A∣B)⋅P(B)=P(B∣A)⋅P(A) . 3. Bayes’s Theorem 예제 . 우리나라 사람이 폐암에 걸릴 확률은 3%이고, 폐암을 99% 양성으로 진단하는 시약이 있다. 이 시약으로 폐암을 진단했을 때 양성반응을 보인 경우, 실제 폐암에 걸렸을 확률은 얼마인가? | . . 우리가 구해야할 확률은 $P(폐암|양성)$ 이고, 문제에서 주어진 조건은, | . P(폐암)=0.03 (=&gt;P(정상)=0.97),P(폐암) = 0.03 space (=&gt; P(정상) = 0.97),P(폐암)=0.03 (=&gt;P(정상)=0.97), . P(양성∣폐암)=0.99 (=&gt;P(양성∣정상)=0.01)P(양성|폐암) = 0.99 space (=&gt; P(양성|정상) = 0.01)P(양성∣폐암)=0.99 (=&gt;P(양성∣정상)=0.01) . . Bayes’s theorem을 이용하면, | . P(폐암∣양성)=P(양성∣폐암)⋅P(폐암)/P(양성)P(폐암|양성) = P(양성|폐암) cdot P(폐암) / P(양성)P(폐암∣양성)=P(양성∣폐암)⋅P(폐암)/P(양성) . P(양성)=P(양성∣정상)⋅P(정상)+P(양성∣폐암)⋅P(폐암)P(양성) = P(양성|정상) cdot P(정상) + P(양성|폐암) cdot P(폐암)P(양성)=P(양성∣정상)⋅P(정상)+P(양성∣폐암)⋅P(폐암) . 이며, 이를 계산하면, $P(양성) = 0.01 * 0.97 + 0.99 * 0.03 = 0.03939109$ | . . 따라서 $P(폐암|양성) = 0.99 * 0.03 / 0.03939109 = 0.7539776127037866$ 이며, 약 75%임. | . 4. Maximum likelihood estimation(MLE) . In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data. This is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. – From wikipedia . MLE는 주어진 관측데이터( $x_1, x_2, dots, x_n$ )에 대해, 어떠한 분포를 가정했을 때, 관측데이터와 가장 유사한 분포를 만드는 파라메터( $ theta$ )를 추정하는 방법입니다. | We pick the distribution family p(·), but don’t know the parameter θ | MLE를 수식으로 표현하면 아래와 같습니다. | . θ^MLE=argmax⁡θp(x1,x2,…,xn∣θ) hat theta_{MLE} = underset{ theta}{ operatorname{argmax}} p(x_1, x_2, dots, x_n| theta)θ^MLE​=θargmax​p(x1​,x2​,…,xn​∣θ) . Assume data is independent and identically distributed (iid). This is written | . xi∼i.i.dp(x∣θ),i=1,…,nx_i overset{i.i.d}{ sim} p(x| theta), i = 1, dots, nxi​∼i.i.dp(x∣θ),i=1,…,n . Writing the density as $p(x|θ)$ , then the joint density decomposes as | . p(x1,x2,…,xn∣θ)=∏i=1np(xi∣θ)p(x_1, x_2, dots, x_n| theta) = prod_{i=1}^n p(x_i | theta)p(x1​,x2​,…,xn​∣θ)=i=1∏n​p(xi​∣θ) . 그리고 다음과 같이 Maximum Likelihood가 되는 파라메터(θ)를 추정할 수 있습니다. | . ∇θp(x1,x2,…,xn∣θ)=∇θ∏i=1np(xi∣θ)=0 nabla_{ theta} p(x_1, x_2, dots, x_n| theta) = nabla_{ theta} prod_{i=1}^n p(x_i | theta) = 0∇θ​p(x1​,x2​,…,xn​∣θ)=∇θ​i=1∏n​p(xi​∣θ)=0 . Logarithm tric : It is complicated to calcuate it directly. So we use the fact that the logarithm is monotonically increasing on R+, and the equality | . argmax⁡θg(y)=argmax⁡θln(g(y)) underset{ theta}{ operatorname{argmax}} g(y) = underset{ theta}{ operatorname{argmax}} ln(g(y))θargmax​g(y)=θargmax​ln(g(y)) . ln(∏ifi)=∑iln(fi)ln( prod_i f_i) = sum_i ln(f_i)ln(i∏​fi​)=i∑​ln(fi​) . 데이터 특징에 따라 선택할 수 있는 분포들은 binomial, poisson, gaussian 등이 있습니다. | . 5. MLE 예제 . 공장에서 10개의 제품을 검사했을 때, 정상이 8개, 불량이 2개인 경우가 관측되었습니다. 이 경우, 우리는 binomial distribution을 가정할 수 있습니다. binomial distribution의 pmf는 다음과 같이 정의됩니다. | . Pr⁡(K=k)=f(k;n,θ)=(nk)θk(1−θ)n−k Pr(K = k) = f(k;n, theta)={n choose k} theta^k(1- theta)^{n-k}Pr(K=k)=f(k;n,θ)=(kn​)θk(1−θ)n−k . 파라메터 θ를 MLE로 추정해보면, | . ∇θ∏i=1np(xi∣θ)=∇θln(∏i=1np(xi∣θ))=∇θln((nk)θk(1−θ)n−k)=0 nabla_{ theta} prod_{i=1}^n p(x_i | theta) = nabla_{ theta} ln( prod_{i=1}^n p(x_i | theta)) = nabla_{ theta} ln({n choose k} theta^k(1- theta)^{n-k}) = 0∇θ​i=1∏n​p(xi​∣θ)=∇θ​ln(i=1∏n​p(xi​∣θ))=∇θ​ln((kn​)θk(1−θ)n−k)=0 . ∇θln(θk(1−θ)n−k)=∇θk⋅ln(θ)+(n−k)⋅ln(1−θ)=kθ−n−k1−θ=0 nabla_{ theta} ln( theta^k(1- theta)^{n-k}) = nabla_{ theta} k cdot ln( theta) + (n-k) cdot ln(1- theta) = displaystyle frac{k}{ theta} - frac{n-k}{1- theta} = 0∇θ​ln(θk(1−θ)n−k)=∇θ​k⋅ln(θ)+(n−k)⋅ln(1−θ)=θk​−1−θn−k​=0 . k(1−θ)−θ(n−k)=0k(1- theta) - theta(n-k) = 0k(1−θ)−θ(n−k)=0 . θ=kn theta = displaystyle frac{k}{n}θ=nk​ . MLE를 통해 불량확률에 파라메터는 θ는, n = 10, k = 2인 경우, 0.2로 추정할 수 있습니다. | multivariate gaussian distribution에 MLE를 적용하면, mean과 covariance는 각각 아래와 같습니다.(계산유도과정은 생략합니다) | . μ^MLE=1n∑i=1nxi, σ^MLE=1n∑i=1n(xi−μ^MLE)(xi−μ^MLE)T hat mu_{MLE} = frac{1}{n} sum_{i=1}^n x_i, space hat sigma_{MLE} = frac{1}{n} sum_{i=1}^n (x_i - hat mu_{MLE})(x_i - hat mu_{MLE})^Tμ^​MLE​=n1​i=1∑n​xi​, σ^MLE​=n1​i=1∑n​(xi​−μ^​MLE​)(xi​−μ^​MLE​)T . If $x_1, dots x_n$ don’t “capture the space” well, $ theta_{MLE}$ can overfit the data. | 관측데이터에 overfitting되지 않는 파라메터(모수) 추정 방법에 대한 문제를 제기하는 것 같습니다. 아마도 뒤에서 Bayesian 방법론을 적용할 듯합니다. | . 6. Gaussian Distribution . 6-1. Univariate Gaussian Distribution . In statistics, a normal distribution (also known as Gaussian, Gauss, or Laplace–Gauss distribution) is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is | . f(x)=1σ2πe−12(x−μσ)2f(x) = frac {1}{ sigma sqrt{2 pi}} e ^ {- frac{1}{2}( frac{x- mu}{ sigma})^2}f(x)=σ2π . ​1​e−21​(σx−μ​)2 . random variable X is normally distributed with mean $ mu$ and standard deviation $ sigma$ , one may write $ displaystyle X sim { mathcal {N}}( mu , sigma ^{2})$ | . . 6-2. Multivariate Gaussian Distribution . The multivariate normal distribution of a k-dimensional random vector $X =(X_{1}, dots ,X_{k})^{T}$ can be written in the following notation: | . X∼Nk(μ,Σ),X sim { mathcal {N}}_{k} ({ boldsymbol { mu }},{ boldsymbol { Sigma }}),X∼Nk​(μ,Σ), . with k-dimensional mean vector | . μ=E⁡[X]=(E⁡[X1],E⁡[X2],…,E⁡[Xk])T,{ boldsymbol { mu }= operatorname {E} [ mathbf {X} ]=( operatorname {E} [X_{1}], operatorname {E} [X_{2}], ldots , operatorname {E} [X_{k}])^{ textbf {T}},}μ=E[X]=(E[X1​],E[X2​],…,E[Xk​])T, . and k x k covariance matrix | . Σi,j=E⁡[(Xi−μi)(Xj−μj)]=Cov⁡[Xi,Xj], Sigma_{i,j}= operatorname {E} [(X_{i}- mu _{i})(X_{j}- mu _{j})]= operatorname {Cov} [X_{i},X_{j}],Σi,j​=E[(Xi​−μi​)(Xj​−μj​)]=Cov[Xi​,Xj​], . such that $1 leq i leq k$ and $1 leq j leq k$ | . . The general form of its probability density function is | . fX(x1,…,xk)=exp⁡(−12(x−μ)TΣ−1(x−μ))(2π)k∣Σ∣{ displaystyle f_{ mathbf {X} }(x_{1}, ldots ,x_{k})={ frac { exp left(-{ frac {1}{2}}({ mathbf {x} }-{ boldsymbol { mu }})^{ mathrm {T} }{ boldsymbol { Sigma }}^{-1}({ mathbf {x} }-{ boldsymbol { mu }}) right)}{ sqrt {(2 pi )^{k}|{ boldsymbol { Sigma }}|}}}}fX​(x1​,…,xk​)=(2π)k∣Σ∣ . ​exp(−21​(x−μ)TΣ−1(x−μ))​ . (참고) Mahalanobis distance The Mahalanobis distance is a measure of the distance between a point P and a distribution D . | . (x−μ)TΣ−1(x−μ) sqrt{({ mathbf {x} }-{ boldsymbol { mu }})^{ mathrm {T} }{ boldsymbol { Sigma }}^{-1}({ mathbf {x} }-{ boldsymbol { mu }})}(x−μ)TΣ−1(x−μ) . ​ . 만약, $ Sigma = sigma^2 I$ 이라면, 즉, 각 변수간 공분산이 모두 0인 경우, 마할라노비스 거리는 유클리디안 거리와 같아집니다. | . 7. Covariance의 의미 . Variance(분산) : 데이터가 펼쳐진 정도, 분산이 작으면, 데이터가 좁은영역에 모여있고, 분산이 크면 데이터가 넓은 영역에 퍼지는 형태를 보임 | . var⁡(X)=Cov⁡(X,X)=E⁡[(X−E⁡[X])(X−E⁡[X])T] operatorname {var} ( mathbf {X} )= operatorname {Cov} ( mathbf {X} , mathbf {X} )= operatorname {E} left[( mathbf {X} - operatorname {E} [ mathbf {X} ])( mathbf {X} - operatorname {E} [ mathbf {X} ])^{ rm {T}} right]var(X)=Cov(X,X)=E[(X−E[X])(X−E[X])T] . Covariance(공분산) : 두 변수간 데이터가 퍼진 정도를 나타냄 | . Cov⁡(X,Y)=E⁡[(X−E⁡[X])(Y−E⁡[Y])T] operatorname {Cov} ( mathbf {X} , mathbf {Y} )= operatorname {E} left[( mathbf {X} - operatorname {E} [ mathbf {X} ])( mathbf {Y} - operatorname {E} [ mathbf {Y} ])^{ rm {T}} right]Cov(X,Y)=E[(X−E[X])(Y−E[Y])T] . Covariance를 의미를 살펴보면, | . Cov(X,Y)=(x1−μx)(y1−μy)+(x2−μx)(y2−μy)+⋯+(xn−μx)(yn−μy)nCov(X, Y) = frac{(x_1 - mu_x)(y_1 - mu_y) + (x_2 - mu_x)(y_2 - mu_y) + dots + (x_n - mu_x)(y_n - mu_y)}{n}Cov(X,Y)=n(x1​−μx​)(y1​−μy​)+(x2​−μx​)(y2​−μy​)+⋯+(xn​−μx​)(yn​−μy​)​ . 이고, $(x_i - mu_x)(y_i - mu_y)$ 가 양수인 경우는 각각 평균보다 크거나, 각각 평균보다 작은 경우이며, 음수인 경우 그 반대이다. 또한 평균으로부터 값이 멀어질 수록 그 값이 커지게 된다. 즉, 각 변수의 평균을 중심으로 분산의 방향을 확인할 수 있다. 데이터를 축에 plotting했을 때, Cov(X,Y)가 양수이면, 1,3분면에 분포하며, 음수이면 주로 2,4분면에 분포할 것으로 생각할 수 있다. 다만, 공분산은 각 변수의 단위에 따라 값의 크기가 달라져서 절대적인 값의 크기로 비교하는 것은 타당하지 않다. | . . 그래서, 공분산을 각각의 표준편차로 나누어 그 값을 [-1, 1]로 변환하여 계산한 것을 correlation(상관계수)라고 한다. | . ρX,Y=corr⁡(X,Y)=Cov⁡(X,Y)σXσY=E⁡[(X−μX)(Y−μY)]σXσY,if σXσY&gt;0.{ displaystyle rho _{X,Y}= operatorname {corr} (X,Y)={ operatorname {Cov} (X,Y) over sigma _{X} sigma _{Y}}={ operatorname {E} [(X- mu _{X})(Y- mu _{Y})] over sigma _{X} sigma _{Y}}, quad { text{if}} sigma _{X} sigma _{Y}&gt;0.}ρX,Y​=corr(X,Y)=σX​σY​Cov(X,Y)​=σX​σY​E[(X−μX​)(Y−μY​)]​,if σX​σY​&gt;0. . 8. Linear Regression . 8. Probability View of Linear Regression . 9. MAP .",
            "url": "https://hyunholee26.github.io/fastpages/gaussian%20process/2022/08/26/Understanding-Gaussian-Process-without-any-knowledge.html",
            "relUrl": "/gaussian%20process/2022/08/26/Understanding-Gaussian-Process-without-any-knowledge.html",
            "date": " • Aug 26, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "AWRA Geospatial Water Technology Conference 2022",
            "content": "AWRA Geospatial Water Technology Conference 2022 . AWRA GWTC 2022 Program . 1. Pysheds: Fast and Interactive Digital Elevation Model Processing in Python . Matthew Bartos, University of Texas, Austin | . 2. Creating Dynamic Water Resources Engineering Reports Using Geospatial Python Libraries and Jupyter Lab . Abdul Raheem Siddiqui, Dewberry; Seth Lawler, Dewberry | . 3. Generate Curve Number for Any Area Within the United States: An Example of How Repetitive GIS Workflows can be Automated Using QGIS and Python . Abdul Raheem Siddiqui, Dewberry | . 4. A Novel Hydrologic Soil Classification Method Using Unsupervised Clustering Techniques and gSSURGO Data to Improve Soil Parameterization in the Conterminous United States . Pin-Ching Li, Purdue University | . 5. Flood Inundation Map Usage Soars During Major Event . David Curtis, WEST Consultants, Inc. | . 6. Evaluating Uncertainty in FEMA Flood Insurance Rate Maps (FIRMs) using Bayesian Model Averaging (BMA) and Hierarchical BMA . Tao Huang, Purdue University | . 7. Flood Potential Index and Its Use in Space-Time Forecasting . Witold Krajewski, University of Iowa | . 8. Effect of Linear and Non-Linear Detrending on Streamflow Data . Jibin Joseph, Purdue University | . 9. How Reliable are Synthetic Rating Curves for Large Scale Flood Mapping? . Ankit Ghanghas, Purdue University | . 10. TrashVision - Efficient AI Trash Observations for Healthy Waterways . Gary Conley, Fuscoe Engineering | . 11 Leveraging High Resolution Impervious Surface Data to Support Stormwater Planning . Brandon Palin, Ecopia Tech | . 12. GeoAI-based Identification of Hydraulic Structures for Improving DEM-based Hydrologic Delineation . Ruopu Li, Southern Illinois University | . 13. A Data-Driven Model for Assessing Nutrient Management Efficacy at the Watershed Scale . Kimia Karimi, North Carolina State University | . 14. Using Independent Component Analysis for Separation of Baseflow, Runoff Volume, and Tidal Input at a Tidal Freshwater Stream Near Charleston, South Carolina . Emma Collins, Robinson Design Engineers | . 15. 3D Hydrography for Southeast Texas . Andrew Brenner, Quantum Spatial Inc; Cathy Power, Quantum Spatial Inc | . 16. Extracting NHD/EDH Features from Digital Elevation Models – a Multi-Pronged Approach . Alvan Karlin, Dewberry; Josh Novac, Dewberry | . 17. Water Here, Water There . Cynthia Ritmiller, USGS | 18. SPRING - An Automated and Flexible Framework for Developing Large-scale 3D Representations of River Network . | Sayan Dey, Purdue University | . 19. Improvements to DEM Cutter in Areas of Voids and Channelized Flow . Brian Gelder, Iowa State University | . 20. Hybrid Stream Feature Extraction Methods . Dean Djokic, Esri | . 21. Extracting River Morphology Features from Single- and Multi-beam Bathymetry Surveys . Chung-Yuan Liang, Purdue University | . 22. RIMORPHIS: River Morphology Information System . Venkatesh Merwade, Purdue University | . 23. Providing Open Access to the Bureau of Reclamation’s Data: Overview and Upcoming Features of the Reclamation Information Sharing Environment (RISE) . Allison Odell, US Bureau of Reclamation, Research and Development Office | . 24. Findable, Accessible, Interoperable, and Reusable Geospatial Data in CUAHSI HydroShare . David Tarboton, Utah State University | . 25. Getting the NHDPlus HR Dataset from Our Hands to Yours: The Process of Delivering High Quality Data to the Public . Hayley Thompson, USGS | . 26. Flood Inundation Mapping-based Control of Stormwater Infrastructure Mitigates Flood Vulnerability . Jeil Oh, University of Texas at Austin | . 27. Developing Customized NRCS Unit Hydrographs for Ungauged Watersheds in Indiana, USA . Tao Huang, Purdue University | . 28. USGS Approach and Treatment of Storm-water Networks in the NHD, WBD, and StreamStats . Roger Barlow, USGS | . 29. Enhancing Near Confluence Infrastructure Resilience across Continental United States using Copula-based Joint Probability Design Criteria . Ankit Ghanghas, Purdue University | . 30. Getting Ready for Climate Change on the NC US74 Corridor: Using Simulation to Measure and Improve Resilience of a Major Transportation Corridor . Stephen Bourne, Atkins North America Inc | . 31. Introducing the 3D National Topography Model and 3D Hydrography Program . Stephen Aichele, USGS | . 32. 3D Hydrography Program Datasets – Modernized Data Model . Kevin McNinch, USGS | . 33. 3D Hydrography Program Datasets – Hydrography and Hydrologic Units Derived from Elevation . Amanda Lowe, USGS | . 34. Hydrologically Enhanced Elevation Surfaces . Christy-Ann Archuleta, USGS | . 35. Urban Flooding Open Knowledge Network: Delivering Flood Information to Anyone, Anytime, Anywhere . Lilit Yeghiazarian, University of Cincinnati | . 36. Achieving an Actionable, Real-time, Continental Flood Information System . Mike Johnson, Lynker | . 37. Developing an Urban Flood Model for Forecasting Flood Inundation Risks Using SWMM for the City of Wilmington, NC . Shiqi Fang, North Carolina State University | . 38. Urban Flooding Open Knowledge Network: Future Tools, Functionalities and Products . Siddharth Saksena, Virginia Tech University | . 39. Open Source Software and the Cloud: Building Highly Effective, Low-cost, Scalable Solutions for Risk Analysis in the Cloud . Seth Lawler, Dewberry; Mat Mampara, Dewberry | . 40. Leveraging Geospatial and Water Data to Provide Opportunities for Reducing Management Risk and Improving Water Security . David Wegner, Woolpert Engineering | . 41. Cloud Native, Data Management Strategies for Multi-Jurisdictional Flood Hazard Characterization and Risk Assessment . Seth Lawler, Dewberry; Mat Mampara, Dewberry | . 42. Advanced Decision-Making Framework and Flood Resiliency Planning Through Integration of Fluid and Solid Systems . Rouzbeh Nazari, University of Alabama, Birmingham | . 43. Assessment of Machine Learning as Scheme for Streamflow Predictions in Ungauged Basins Using Monte Carlo Analysis . Pin-Ching Li, Purdue University | . 44. Use of LiDAR and Synthetic Bathymetry to Develop Stream Cross Sections for Alternative Modeling of Streamflow Using HEC-RAS . James Sullivan, University of North Dakota | . 45. Calculating the Water Storage Required to Mitigate Land Use Change in the Lower Fox River Basin, Wisconsin . Jeremy Freund, Outagamie County Land Con | . 46. Evaluations of Streambed Scours near Bridge Piers using HYCAT and LiDAR Data . Vida Atashi, University of North Dakota; Yeo Howe Lim, University of North Dakota | . 47. Assessing Water Quality of English Coulee and Flood Control Measures using HEC-RAS Model with LiDAR derived Data . Michael Rosati, University of North Dakota | . 48. Delineating Lake Watersheds on a Low-Lying Carbonate Island: A Comparative Analysis Between the ArcGIS Spatial Analyst and Arc Hydro Extensions . Natalie Salazar, University of South Florida | .",
            "url": "https://hyunholee26.github.io/fastpages/geospatial%20water%20technology/2022/05/01/awra-gwtc.html",
            "relUrl": "/geospatial%20water%20technology/2022/05/01/awra-gwtc.html",
            "date": " • May 1, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "ESA Digital Twin Hydrology",
            "content": "1. ESA Digital Twin Hydrology . . 2. Fully Automated DEM Generation using Satellite Imagery . . 3. generate reservoir river dem using satellite image bathemetry . http://www.jlakes.org/uploadfile/news_images/hpkx/2020-11-12/2020WR027147.pdf .",
            "url": "https://hyunholee26.github.io/fastpages/digital%20twin%20hydrology/2022/05/01/ESA-Digital-Twin-Hydrology.html",
            "relUrl": "/digital%20twin%20hydrology/2022/05/01/ESA-Digital-Twin-Hydrology.html",
            "date": " • May 1, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Useful expressions in English",
            "content": "2022-03-23 . inform : 목적어로 정보를 전달하고자 하는 사람이 와야한다. inform 사람 of 정보 inform 사람 that My daughter informed me that she was pregnant Please inform me of the result of the game . notify : inform과 형태는 같으나 공식적으로 정보를 전달하는 경우 사용된다 notify 사람 of 정보 notify 사람 that The committee’s decision will be notified to all employees Competition winners will be notified by post . announce : 목적어로 전달하고자 하는 정보가 와야한다. He has announced his intention to retire Please announce to me the result of the game . be familiar with : ~ 를 잘 알다 -&gt; have some knowledge, knowing someone = be acquainted with I am familiar with the reporter. . be used to : ~ 에 익숙하다 -&gt; have some experience = be accustomed to I am not used to eating alone. . 2022-03-12 . How is it going(=coming along)? | I am afraid that it is diffcult to finish it as scheduled(=as we planned). | This is a top priority. Mr.Kyle wants the document this(every, last) week. | After this meeting, I will have Jane finish my other work. | If you have to, do it. | Do not hesitate to contact me if you have any question about(=regarding) this work. | I would appreciate it if you would let me know before you send it to him. tell me : 샅샅히 알려주다 | let me know : 변경사항만 알려주다 | . | Help yourself(많이 퍼가세요, 상황에 맞게 사용해야함), Enjoy your meal 이 오히려 적합할 수 있음 | What are you thinking about? | I was thiking of calling Paul(=give Paul a call, make a phone call). about : 전반적인 사항에 관해 | of : 구체적인 아이디어 | on : 전문적인 사항 | . | I have a presentation on the 29th, and I have to prepare with Paul. prepare : 프로젝트 준비 | get ready : 동작을 할 준비, get ready for dinner, get ready for bed | . | You are lucky. | Paul is good at computers. | Good for you : 잘됐다 | That is good for you : 몸에 좋아 | I am relieved, That is a relief, I am glad : 다행이다 | I was worried about computer work. (a work : 예술작품) | I need to get to know Paul better. get close : get close to each other (물리적 거리) | be close : I am close to Selley. | get to know (someone) | . | That is a good idea but I do not think (that) I can go. | I an going skiing tomorrow, so I have to do a lot of work will : 보통 말하는 순간에 결정할때 사용 | be going to : 예정된 일에 사용 | go (Verb)ing : 활동을 하러가다, go walking, go swimming, go skiing | I did a lot of reading, I did little reading. | . | Do you know any good bars near our office? | I will text(email, katok) you the address later. | I made a reservation for(목적지를 바라보는 전치사) seven o’clock tomorrow at a famous Korean restaurant. make a reservation : 자리를 미리 맡아놓다 | reserve + something : I would like to reserve three tables. | make an appointment : 서비스를 받을 예약을 하다 | . | Have you ever eaten Korean food? | I have never eaten(=had, tried) Korean food | How does it taste? (먹어본 경우) | What is it like(안먹어본경우, 어때요?) | Korean food is usually salty and a little spicy. | Can you eat spicy food? well : 잘 하는것 | . | Yes(Sure, Of course), I can eat everything. =&gt; 남기지 않고 싹 다 먹는다. | Yes, I can eat almost anything. I am not a picky either. : 아무거나 다 먹는다 | I am picky about food. : 입맛이 까다로워요 | That is great, Terrific, Perfect | If you want, I will order for you. instead of : 하나를 포기하는 개념이 포함됨 | on behalf of : ~ 를 대표하여 | . | Tonight, I am going with Selley to the restaurant you recommended last time. will : 말하는 순간에 결정 | be going to : 예정된 미래의 일 | . | The restaurant is very famous for its traditional Korean food. be famous for (소유격) : She is famous for her beautiful voice. | . | I know, I am looking forward toit. | I cannot wait to see you, I am really looking forward to seeing(meeting) you. | Have you ever used chopsticks before? | I used them a few times when I was in Japan. | Are they different from Japanese chopsticks? | What is the difference between Korean chopsticks and Japanese chopsticks? | Korean chopstick are thinner and made of steel. | I am worried that (~할까봐 걱정하다) | I am worried because I haven’t been feeling well these days. | Health is the most important things | What’s the matter? | What’s the problem? | My health is not as good as before | My health is not as good as it used to be | I am not as healthy as I used to be / before | I get tired easily | Do you exercise? work out : 근육운동 | . | Since I started exercising, I have been feeling much(= a lot) better. | I have been exercising these days, and I love it so much. | I quit exercising because of the air is dirty. to 부정사 : ~ 하기 위해 멈추다(미래의미) | -ing : ~ 해 오던 것을 멈추다(과거~미래) | . | I just exercise at home in the house : 물리적인 집 | at home : 나의 생활 공간으로써의 집 | . | Thirty minutes a day is enough. 시간이나 돈의 양은 단수 취급 | . | You’re right, the air pollution is just an excuse. | I should try to lose weight. | I will get started today(on Tuesday) | How have you been, How are you doing? | I heard that you are moving to the US. (제 3자로부터 들었을때) | It all happened so fast. | Things happened so suddenly | Everything happened all of sudden. | I still can’t believe it | I am still trying to wrap my head around it.(아직도 믿기지 않아) | Do you need a help?/ Can I help you? (상대방이 어떤 도움이 필요한지 알고 있을때, 보일때 사용) | Is there anything I can do for you? | Is there anything you need? | I am okay, I am fine : 호의에 대한 거절 | That sounds good. : 좋아! | Thanks for saying that. | Thanks for asking (그렇게 말해줘서 고마워) | Let me know if there’s anything I can do for you (you need) | Don’t hesitate to ask if you need any help. | I hope everty thing goes well. | I wish you the best. | How do you like living in America? | I like the fact that the air is clean. | I like the clean air. | I miss my family and friend | I you have time, When you have time, If you can, When you can | come visit me | I will go see you | I wish I could (go visit you) | I am busy, I have a lot of work. | Have you heard of Squid game? | I have heard a lot about you | It is a lot of fun. (어떤 행동 동작이 재미있는) | It’s really good, It’s really interesting(영화등이 재미있을때) | I am binge-watching it | I am hooked after one episode. | I haven’t seen it yet : 아직 안봤다 see : 어떤 존재를 인지(aware)하다 | watch : 정지상태에서 무엇가를 지켜보다 | . | It seems to be very popular among America. | The show is a big hit in American. | I can’t watch it because I am too busy with(because of를 쓰지 않는다) work and kids. | I am so busy that I can’t watch it | I am too busy to watch it | Why don’t you watch it after putting your kids to bed. make : 억지로/강제로 하다 | . | I am so tired at night that I go to bed at the same time as my kids | That’s too bad. You would like it. : 안타깝다 니가 좋아할텐데 | I got my hair cut. | I got my car washed. | I got my coat cleaned | I got everything done. | I got my suit made. | I have to get my passport picture taken | I will get your watch fixed | Would you mind | . 쓰는영어 .",
            "url": "https://hyunholee26.github.io/fastpages/english/2022/03/12/useful-expressions-in-english.html",
            "relUrl": "/english/2022/03/12/useful-expressions-in-english.html",
            "date": " • Mar 12, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Sungyong Seo's talks in youtube",
            "content": "1. Effective Feature Learning based on Deep Networks for Urban Heat Island Prediction . . I will summarize it later | . 2. Graph Networks with Physics-aware Knowledge Informed in Latent Space . . The topic of this paper is similar to my research interest. I consider that this paper could be the starting point of my research. | I will summarize it later | . 3. Sungyong Seo’s Google Scholar .",
            "url": "https://hyunholee26.github.io/fastpages/physics%20guided%20deep%20learning/2022/02/27/Sungyong-Seo-talks-in-youtube.html",
            "relUrl": "/physics%20guided%20deep%20learning/2022/02/27/Sungyong-Seo-talks-in-youtube.html",
            "date": " • Feb 27, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "A Simple Framework for Contrastive Learning of Visual Representations",
            "content": "1. A Simple Framework for Contrastive Learning of Visual Representations . Authors : Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton | Abstract : This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels. | .",
            "url": "https://hyunholee26.github.io/fastpages/contrastive%20learning/2022/02/24/simclr.html",
            "relUrl": "/contrastive%20learning/2022/02/24/simclr.html",
            "date": " • Feb 24, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Steve Brunton's talks in youtube",
            "content": "1. Sparse Identification of Nonlinear Dynamics (SINDy): Sparse Machine Learning Models 5 Years Later! . . SINDy is a kind of solver to find equations governed dynamics by combining many terms sparsely. Also, he suggests 4~5 key questions to find the equation for dynamics in terms of data, combined terms, optimization, etc. | . 2. Deep Learning to Discover Coordinates for Dynamics: Autoencoders &amp; Physics Informed Machine Learning . . He explains that deep learning, especially deep autoencoder, is a method to discover proper coordinate systems for the dynamics. | .",
            "url": "https://hyunholee26.github.io/fastpages/physics%20guided%20deep%20learning/2022/02/24/Steve-Brunton-talks-in-youtube.html",
            "relUrl": "/physics%20guided%20deep%20learning/2022/02/24/Steve-Brunton-talks-in-youtube.html",
            "date": " • Feb 24, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Rainfall Runoff Modeling using Recurrent Neural Network",
            "content": "1. Rainfall–runoff modelling using Long Short-Term Memory (LSTM) networks . Authors : Frederik Kratzert, Daniel Klotz, Claire Brenner, Karsten Schulz, and Mathew Herrnegger | Abstract : Rainfall–runoff modelling is one of the key challenges in the field of hydrology. Various approaches exist, ranging from physically based over conceptual to fully data-driven models. In this paper, we propose a novel data-driven approach, using the Long Short-Term Memory (LSTM) network, a special type of recurrent neural network. The advantage of the LSTM is its ability to learn long-term dependencies between the provided input and output of the network, which are essential for modelling storage effects in e.g. catchments with snow influence. We use 241 catchments of the freely available CAMELS data set to test our approach and also compare the results to the well-known Sacramento Soil Moisture Accounting Model (SAC-SMA) coupled with the Snow-17 snow routine. We also show the potential of the LSTM as a regional hydrological model in which one model predicts the discharge for a variety of catchments. In our last experiment, we show the possibility to transfer process understanding, learned at regional scale, to individual catchments and thereby increasing model performance when compared to a LSTM trained only on the data of single catchments. Using this approach, we were able to achieve better model performance as the SAC-SMA + Snow-17, which underlines the potential of the LSTM for hydrological modelling applications. | My Notes : Figure 15 shows the evolution of a single LSTM cell (ct; see Sect. 2.1) of a trained LSTM over the period of one input sequence (which equals 365 days in this study) for an arbitrary, snow-influenced catchment. We can see that the cell state matches the dynamics of the temperature curves, as well as our understanding of snow accumulation and snowmelt. As soon as temperatures fall below 0 ∘C the cell state starts to increase (around time step 60) until the minimum temperature increases above the freezing point (around time step 200) and the cell state depletes quickly. Also, the fluctuations between time steps 60 and 120 match the fluctuations visible in the temperature around the freezing point. Thus, albeit the LSTM was only trained to predict runoff from meteorological observations, it has learned to model snow dynamics without any forcing to do so. | . . 2. Sequence-to-Sequence Learning with Deep Neural Networks in Rainfall-Runoff Modeling in Iowa . Authors : Xiang, Z., Demir, I. | Abstract : Sequence-to-sequence(seq2seq) learning with deep neural networks can be used to solve complex time-series problems. This study presents an end-to-end rainfall-runoff model considering the NCEP/CPC 4km precipitation, empirical evapotranspiration, and USGS stream runoff data in watershed scale using seq2seq learning with Gated Recurrent Unit network. For each USGS station-based watershed, after calibration with appropriate input timesteps and batch size determined from the domain, the model can predict runoff for the next 120 hours with observed precipitation and runoff, empirical evapotranspiration, and forecast precipitation. For the downstream stations, we used the upstream forecast results as an additional input, which reduced the errors caused by spatial inequality of precipitation and improved the model accuracy. Final evaluation shows that, for the test water year 2018, on 126 available USGS stations in Iowa, the median of Nash-Sutcliffe model Efficiency (NSE) are 0.84, 0.77, 0.72, 0.69 and 0.67 for the prediction of 24th, 48th, 72nd, 96th and 120th hour, while the stream persistences are 0.74, 0.40, 0.11, -0.02 and -0.15 respectively. 98 out of 126 stations have an acceptable 120th-hour prediction NSE value over 0.50. The results show strong predictive power and could be used to improve forecast accuracy in short-term flood forecast applications. This study also demonstrates a strong potential of applying seq2seq learning on time-series tasks in hydrology and earth science studies. | My Notes : For each USGS station-based watershed, after calibration with appropriate input timesteps and batch size determined from the domain, the model can predict runoff for the next 120 hours with observed precipitation and runoff, empirical evapotranspiration, and forecast precipitation. For the downstream stations, we used the upstream forecast results as an additional input, which reduced the errors caused by spatial inequality of precipitation and improved the model accuracy. | . 3. A Rainfall-Runoff Model With LSTM-Based Sequence-to-Sequence Learning . Authors : Zhongrun Xiang, Jun Yan, Ibrahim Demir | Abstract : Rainfall-runoff modeling is a complex nonlinear time series problem. While there is still room for improvement, researchers have been developing physical and machine learning models for decades to predict runoff using rainfall data sets. With the advancement of computational hardware resources and algorithms, deep learning methods such as the long short-term memory (LSTM) model and sequence-to-sequence (seq2seq) modeling have shown a good deal of promise in dealing with time series problems by considering long-term dependencies and multiple outputs. This study presents an application of a prediction model based on LSTM and the seq2seq structure to estimate hourly rainfall-runoff. Focusing on two Midwestern watersheds, namely, Clear Creek and Upper Wapsipinicon River in Iowa, these models were used to predict hourly runoff for a 24-hr period using rainfall observation, rainfall forecast, runoff observation, and empirical monthly evapotranspiration data from all stations in these two watersheds. The models were evaluated using the Nash-Sutcliffe efficiency coefficient, the correlation coefficient, statistical bias, and the normalized root-mean-square error. The results show that the LSTM-seq2seq model outperforms linear regression, Lasso regression, Ridge regression, support vector regression, Gaussian processes regression, and LSTM in all stations from these two watersheds. The LSTM-seq2seq model shows sufficient predictive power and could be used to improve forecast accuracy in short-term flood forecast applications. In addition, the seq2seq method was demonstrated to be an effective method for time series predictions in hydrology. | .",
            "url": "https://hyunholee26.github.io/fastpages/rainfall-runoff%20modeling/recurrent%20neural%20network/2022/02/20/Rainfall-Runoff-Modeling-using-Recurrent-Neural-Network.html",
            "relUrl": "/rainfall-runoff%20modeling/recurrent%20neural%20network/2022/02/20/Rainfall-Runoff-Modeling-using-Recurrent-Neural-Network.html",
            "date": " • Feb 20, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Rainfall Runoff Modeling using Graph Neural Network",
            "content": "1. High-resolution rainfall-runoff modeling using graph neural network . Authors : Zhongrun Xiang, Ibrahim Demir | Abstract : Time-series modeling has shown great promise in recent studies using the latest deep learning algorithms such as LSTM (Long Short-Term Memory). These studies primarily focused on watershed-scale rainfall-runoff modeling or streamflow forecasting, but the majority of them only considered a single watershed as a unit. Although this simplification is very effective, it does not take into account spatial information, which could result in significant errors in large watersheds. Several studies investigated the use of GNN (Graph Neural Networks) for data integration by decomposing a large watershed into multiple sub-watersheds, but each sub-watershed is still treated as a whole, and the geoinformation contained within the watershed is not fully utilized. In this paper, we propose the GNRRM (Graph Neural Rainfall-Runoff Model), a novel deep learning model that makes full use of spatial information from high-resolution precipitation data, including flow direction and geographic information. When compared to baseline models, GNRRM has less over-fitting and significantly improves model performance. Our findings support the importance of hydrological data in deep learning-based rainfall-runoff modeling, and we encourage researchers to include more domain knowledge in their models. | . 2. Fully distributedrainfall-runoff modeling using spatial-temporal graph neural network . Authors : Zhongrun Xiang, Ibrahim Demir | Abstract : Recent studies using latest deep learning algorithms such as LSTM (Long Short-Term Memory) have shown great promise in time-series modeling. There are many studies focusing on the watershed-scale rainfall-runoff modeling or streamflow forecasting, often considering a single watershed with limited generalization capabilities. To improve the model performance, several studies explored an integrated approach by decomposing a large watershed into multiple sub-watersheds with semi-distributed structure. In this study, we propose an innovative physics-informed fully-distributed rainfall-runoff model, NRM-Graph (Neural Runoff Model-Graph), using Graph Neural Networks (GNN) to make full use of spatial information including the flow direction and geographic data. Specifically, we applied a time-series model on each grid cell for its runoff production. The output of each grid cell is then aggregated by a GNN as the final runoff at the watershed outlet. The case study shows that our GNN based model successfully represents the spatial information in predictions. NRM-Graph network has shown less over-fitting and a significant improvement on the model performance compared to the baselines with spatial information. Our research further confirms the importance of spatially distributed hydrological information in rainfall-runoff modeling using deep learning, and we encourage researchers to incorporate more domain knowledge in modeling. | . 3. Large-scale river network modeling using Graph Neural Networks . Authors : Frederik Kratzert, Daniel Klotz, Martin Gauch, Christoph Klingler, Grey Nearing, and Sepp Hochreiter | Abstract : In the recent past, several studies have demonstrated the ability of deep learning (DL) models, especially based on Long Short-Term Memory (LSTM) networks, for rainfall-runoff modeling. However, almost all of these studies were limited to (multiple) individual catchments or small river networks, consisting of only a few connected catchments. In this study, we investigate large-scale, spatially distributed rainfall-runoff modeling using DL models. Our setup consists of two independent model components: One model for the runoff-generation process and one for the routing. The former is an LSTM-based model that predicts the discharge contribution of each sub-catchment in a river network. The latter is a Graph Neural Network (GNN) that routes the water along the river network network in hierarchical order. The first part is set up to simulate unimpaired runoff for every sub-catchment. Then, the GNN routes the water through the river network, incorporating human influences such as river regulations through hydropower plants. The main focus is to investigate different model architectures for the GNN that are able to learn the routing task, as well as potentially accounting for human influence. We consider models based on 1D-convolution, attention modules, as well as state-aware time series models.The decoupled approach with individual models for sub-catchment discharge prediction and routing has several benefits: a) We have an intermediate output of per-basin discharge contributions that we can inspect. b) We can leverage observed streamflow when available. That is, we can optionally substitute the discharge simulations of the first model with observed discharge, to make use of as much observed information as possible. c) We can train the model very efficiently. d) We can simulate any intermediate node in the river network, without requiring discharge observations.For the experiments, we use a new large-sample dataset called LamaH (Large-sample Data for Hydrology in Central Europe) that covers all of Austria and the foreign upstream areas of the Danube. We consider the entire Danube catchment upstream of Bratislava, a highly diverse region, including large parts of the Alps, that covers a total area of more than 130000km2. Within that area, LamaH contains hourly and daily discharge observations for more than 600 gauge stations. Thus, we investigate DL-based routing models not only for daily discharge, but also for hourly discharge.Our first results are promising, both daily and hourly discharge simulation. For example, the fully DL-based distributed models capture the dynamics as well as the timing of the devastating 2002 Danube flood. Building upon our work on learning universal, regional, and local hydrological behaviors with machine learning, we try to make the GNN-based routing as universal as possible, striving towards a globally applicable, spatially distributed, fully learned hydrological model. | ppt slide | . 4. Short-term Hourly Streamflow Prediction with Graph Convolutional GRU Networks . Authors : Muhammed Sit, Bekir Demiray, Ibrahim Demir | Abstract : The frequency and impact of floods are expected to increase due to climate change. It is crucial to predict streamflow, consequently flooding, in order to prepare and mitigate its consequences in terms of property damage and fatalities. This paper presents a Graph Convolutional GRUs based model to predict the next 36 hours of streamflow for a sensor location using the upstream river network. As shown in experiment results, the model presented in this study provides better performance than the persistence baseline and a Convolutional Bidirectional GRU network for the selected study area in short-term streamflow prediction. | .",
            "url": "https://hyunholee26.github.io/fastpages/graph%20neural%20network/rainfall-runoff%20modeling/2022/02/20/Rainfall-Runoff-Modeling-using-Graph-Neural-Network.html",
            "relUrl": "/graph%20neural%20network/rainfall-runoff%20modeling/2022/02/20/Rainfall-Runoff-Modeling-using-Graph-Neural-Network.html",
            "date": " • Feb 20, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "A Comprehensive Survey on Graph Neural Networks",
            "content": "0. Paper Link . A Comprehensive Survey on Graph Neural Networks . 1. Background . This paper will be summarized after reviewing… .",
            "url": "https://hyunholee26.github.io/fastpages/graph%20neural%20network/2022/02/20/A-Comprehensive-Survey-on-Graph-Neural-Networks.html",
            "relUrl": "/graph%20neural%20network/2022/02/20/A-Comprehensive-Survey-on-Graph-Neural-Networks.html",
            "date": " • Feb 20, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Regarding Deep learning with Uncertainty Estimation",
            "content": "I will summarize serveral articles regarding deep learning with uncertainty estimation. Providing confidence intervals in regression problems is related to uncertainty estimation. A phrase that can express this well is quoted as follows. . We do a lot of regression, with everything from random forests to recurrent neural networks. And as good as our models are, we know they can never be perfect. Therefore, whenever we provide our customers with predictions, we also like to include a set of confidence intervals: what range around the prediction will the actual value fall within, with (e.g.) 80% confidence? . 1. A guide to generating probability distributions with neural networks . This article give the example code related to generate distribution of predicted value with cumstomed layer and loss function. | . 2. How to generate neural network confidence intervals with Keras . This article suggests same notion and implementation with Monte Carlo Dropout. Also, it gives the way to calculate optimal dropout rate. | .",
            "url": "https://hyunholee26.github.io/fastpages/uncertainty%20estimation/2022/02/13/Regarding-Deep-learning-with-Uncertainty-Estimation.html",
            "relUrl": "/uncertainty%20estimation/2022/02/13/Regarding-Deep-learning-with-Uncertainty-Estimation.html",
            "date": " • Feb 13, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Building own blog with Jekyll and Github in 20 minutes",
            "content": "Step 0. Pre-requisites . Join Github | Select and download Jekyll theme in http://jekyllthemes.org/ | Create new repository and set it for homepage, and then upload the files of Jekyll theme into the new repository | . Step 1. Edit /_config.yml . This file is relevant to theme, Site settings(title, logo, etc), Site Author(name, bio, social links) | . Step 2. Edit ‘/_data/navigation.yml’ . This file is relevant to menus and its link | Each menus consists of title and url. title is the name of menu and url means a file located in _pages folder. You can set the file with checking the example at /test/_data/navigation.yml | . Step 3. Create ‘/_page’ folder and menu files . This is relevant to creating menu files. This step is connected with previous step. | Menu file could be .md or .html and you can make the files with refering to the example at /test/_pages | . Step 4. Create ‘/_post’ folder and .md files . This is relevant to posting an article in your blog | The name of .md file format must be YYYY-MM-DD-POST-TITLE.md and you can compose the article with markdown. | . Step 5. That’s it . You can check your blog and posts that you wrote. | .",
            "url": "https://hyunholee26.github.io/fastpages/github/jekyll/2022/02/06/Building-own-blog-with-jekyll-and-github-in-20-minutes.html",
            "relUrl": "/github/jekyll/2022/02/06/Building-own-blog-with-jekyll-and-github-in-20-minutes.html",
            "date": " • Feb 6, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://hyunholee26.github.io/fastpages/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Research Interests . Main research interests are below: . Physics-guided machine learning | Spatiotemporal data mining (especially forecasting) | Uncertainty Estimation | . My research focuses on incorporating prior knowledge (especially physics-based knowledge) within data-driven models or learning process as an inductive bias, thereby achieving efficient learning from few samples or sparse observations and making the model easier to understand for scientists and non-machine-learning experts. Also, I would like to research the applications of these techniques in water resources management such as the prediction of water level and dam inflow. For more information about my research experience, please refer to my Curriculum Vitae (PDF). . Education . KAIST (Korea Advanced Institute of Science and Technology), Daejeon, Republic of Korea M.S in Computer Science | Mar. 2008 – Feb. 2010 | . | Ajou University, Suwon, Republic of Korea B.E in Information and Computer Engineering (intensive major course) | Mar. 2001 – Aug. 2007 | . | . Publication . J Park, H Lee (2020) “Prediction of high turbidity in rivers using LSTM algorithm”. Journal of Korean Society of Water and Wastewater 34 (1), 35-43, https://doi.org/10.11001/jksww.2020.34.1.035 . | J Kim, M Park, Y Yoon, H Lee (2020) “Application of recurrent neural network for inflow prediction into multi-purpose dam basin”. Advances in Hydroinformatics, 397-408, https://doi.org/10.1007/978-981-15-5436-0_31 . | J Park, H Lee, CY Park, S Hasan, TY Heo, WH Lee (2019) “Algal morphological identification in watersheds for drinking water supply using neural architecture search for convolutional neural network”. Water 11 (7), 1338, https://doi.org/10.3390/w11071338 . | H Lee, K Wohn (2010) “The layer-based vector texture for 3D rendering”. Proceeding of 2010 Conference on the HCI Society of Korea, 40-43 . | . Work Experience . Korea Water Resources Corporation (K-water), Daejeon, Korea, Jul. 2010 – Present . Senior Manager, Digital Water Platform Dept., Water Platform Development Team, Jan. 2021 – Jun.2022 | Manager, Digital Innovation Dept., Big Data Business Team, Jan. 2020 – Dec. 2020 | Manager, Data Center Dept., Big Data Business Team, Jan. 2019 – Dec. 2019 | Manager, Water Data Collection and Analysis Dept., Water Data Integration Team, Jan. 2018 – Dec. 2018 | Manager, Human Resources Management Dept., HR Management Team, Jan. 2013 – Dec. 2017 | Staff, Information System Management Dept., Information Planning Team, Jul. 2010 – Dec. 2012 | . | . Honors and Awards . 1st Place Prize, 5th Bigdata analysis competition in K-water, Oct. 2021 . | Academic Conference Paper Award, Korean Society of Environmental Engineering Annual Conference, Nov. 2020 . | Bronze Award, ACM-ICPC (International Collegiate Programming Contest) Asia-Seoul Regional, Nov. 2003 . | . Certification . Advanced Data Analytics Professional, certificated by K-Data, Korea, Apr. 2019 (pass rate: 2.76%) | . Professional Skills . Programming Languages : Python, R, C/C++, JAVA, ABAP (SAP) . | Data Science and Machine Learning : Keras, Tensorflow, Sci-kit Learn . | Visualization : Matplotlib, Plotly, Leaflet, QGIS, OpenGL . | .",
          "url": "https://hyunholee26.github.io/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hyunholee26.github.io/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}